{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling\n",
    "\n",
    "* count real occlusions over the whole dataset\n",
    "\n",
    "```\n",
    "Average length of occlusions: 2.92 frames\n",
    "Average number of occluded frames per video: 24.15 frames\n",
    "Average percentage of occluded frames per video: 3.17%\n",
    "Total number of occluded frames: 2994 out of total for the dataset 101156 \n",
    "```\n",
    "\n",
    "* create X (three?) times as many fake occlusions with ground truth data\n",
    "\n",
    "* \"Target\" for real occlusions: \n",
    "\n",
    "1. random numbers\n",
    "2. linear interpolations\n",
    "3. --> cubic interpolations\n",
    "\n",
    "* ✅ add noise\n",
    "* ✅ create DataLoader object\n",
    "* define train-val-test split\n",
    "* create bi-LSTM object\n",
    "* create training loop\n",
    "* create testing loop\n",
    "* create predict function for one video\n",
    "* save sample .csv of blendshapes for all models (3 so far)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import ArtistAnimation\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import interpolate\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "# Standardization functions\n",
    "def fit_and_standardize(data):\n",
    "    shape = data.shape\n",
    "    flat = data.reshape((-1, data.shape[-1]))\n",
    "    scaler = StandardScaler().fit(flat)\n",
    "    scaled = scaler.transform(flat).reshape(shape)\n",
    "    return scaled, scaler\n",
    "\n",
    "\n",
    "def standardize(data, scaler):\n",
    "    shape = data.shape\n",
    "    flat = data.reshape((-1, data.shape[-1]))\n",
    "    scaled = scaler.transform(flat).reshape(shape)\n",
    "    return scaled\n",
    "\n",
    "\n",
    "def inv_standardize(data, scaler):\n",
    "    shape = data.shape\n",
    "    flat = data.reshape((-1, data.shape[-1]))\n",
    "    scaled = scaler.inverse_transform(flat).reshape(shape)\n",
    "    return scaled\n",
    "\n",
    "\n",
    "# Data processing function\n",
    "def process_data(data):\n",
    "    gaps = np.isnan(data)\n",
    "    all_null = np.sum(np.sum(gaps, axis=1), axis=1) > gaps.shape[1] * gaps.shape[2] * 0.8\n",
    "    data = data[~all_null, :, :]\n",
    "    gaps = gaps[~all_null, :, :]\n",
    "    keep = ~np.isnan(np.sum(data, axis=(1, 2)))\n",
    "    return data[keep, :, :], gaps[keep, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import ArtistAnimation\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import interpolate\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, data, labels=None, smoothed_velocities=None, occ_prob=0.3, noise_std=0.01):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data (np.array): Ground truth data of shape (num_videos, num_frames, blendshapes_dim). 125 videos, F frames, 51 features\n",
    "            labels (np.array): Binary occlusion labels of shape (num_videos, num_frames).  (125, F)\n",
    "            smoothed_velocities (np.array): Smoothed velocity for each frame of the video (num_videos, num_frames).  (125, F)\n",
    "            occ_prob (float): Probability of adding synthetic occlusions.\n",
    "            noise_std (float): Standard deviation of Gaussian noise to add to data.\n",
    "        \"\"\"\n",
    "        self.data = data.copy()\n",
    "        self.labels = labels.copy()\n",
    "        self.smoothed_velocities = smoothed_velocities.copy()\n",
    "        self.occ_prob = occ_prob\n",
    "        self.noise_std = noise_std\n",
    "\n",
    "        # Add Gaussian noise to the entire dataset\n",
    "        self.data = self.add_noise(self.data)\n",
    "\n",
    "        self.data, self.labels, self.mask = self.insert_fake_occlussions() \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def insert_fake_occlussions(self):\n",
    "        data = self.data.copy()  # (125 videos, F frames, 51 features)\n",
    "        labels = self.labels.copy()\n",
    "        # create one hot vector that says which data is real 1 and which data is fake 0\n",
    "        # mask = np.ones_like(labels, dtype=int)  # 1 for real, 0 for synthetic\n",
    "        mask = [[1] * len(video) for video in data]\n",
    "\n",
    "        total_frames = sum([len(video) for video in data])\n",
    "        occ_p_init = sum([sum(video) for video in labels]) / total_frames\n",
    "        print(f\"\\nPercentage of REAL occlusions: {occ_p_init}\\n\")\n",
    "        \n",
    "        for video_i, video in enumerate(data):\n",
    "            len_video = len(video)\n",
    "            average_velocity = np.mean(self.smoothed_velocities[video_i])\n",
    "            std_velocity = np.std(self.smoothed_velocities[video_i])\n",
    "            print(f\"Video {video_i} length: {len_video}\")\n",
    "            for frame_n, frame in enumerate(video):\n",
    "\n",
    "                # if the current frame velocity is higher than the average + 1 std, then add an occlusion with a probability\n",
    "                if self.smoothed_velocities[video_i][frame_n] > average_velocity + std_velocity:\n",
    "                    \n",
    "                    if np.random.rand() < self.occ_prob:\n",
    "                        occ_len = int(np.clip(np.random.normal(2.92, 12.05), 2, 24))   # mean and std from the dataset\n",
    "                        # print(f\"Occ len: {occ_len}\")\n",
    "                        # start = np.random.randint(0, len_video - occ_len)\n",
    "                        start = frame_n\n",
    "                        end = start + occ_len\n",
    "                        if end + 1 >= len_video:\n",
    "                            end = len_video - 2\n",
    "                        # check if the occlusion is not already there and update mask only where there wasn't an occlusion already\n",
    "                        mask_update_indices = (np.array(labels[video_i][start:end+1]) != 1)\n",
    "                        # print(f\"Mask update indices: {mask_update_indices}\")\n",
    "                        mask[video_i][start:end+1] = [0 if mask_update_indices[k] else el for k, el in enumerate(mask[video_i][start:end+1])]  # update mask with zeros where fake occlusions are\n",
    "                        labels[video_i][start:end+1] = [1 if mask_update_indices[k] else el for k, el in enumerate(labels[video_i][start:end+1])] # update labels to have 1 for occlusions\n",
    "                        # do nothing to the `data` as it stands for ground truth\n",
    "\n",
    "        # total percentage of all occlusions after inserting fake ones \n",
    "        occ_p = sum([sum(video) for video in labels]) / total_frames\n",
    "        print(f\"\\nPercentage of occlusions: {occ_p}\\n\")\n",
    "        # percentage of masked data\n",
    "        mask_p = 1 - (sum([sum(video) for video in mask]) / total_frames)\n",
    "        print(f\"\\nPercentage of masked data: {mask_p}\\n\")\n",
    "        return data, labels, mask\n",
    "    \n",
    "    def add_noise(self, data):\n",
    "        # Add Gaussian noise across all frames in each video\n",
    "        all_blendshapes = []\n",
    "        for video in data:\n",
    "            blendshapes_in_video = []\n",
    "            for frame in video:\n",
    "                # order the keys in dict `frame` alphabetically\n",
    "                frame = dict(sorted(frame.items()))\n",
    "                blendshapes = frame.values()  # 51 features\n",
    "                if len(blendshapes) != 51:\n",
    "                    print(\"ERROR: Number of blendshapes is not 51\")\n",
    "                    blendshapes = [0.0] * 51\n",
    "                blendshapes = np.array(list(blendshapes))\n",
    "                blendshapes_in_video.append(blendshapes)\n",
    "            # Apply noise across all frames in the video\n",
    "            blendshapes_in_video = np.array(blendshapes_in_video)  # F, 51\n",
    "            print(blendshapes_in_video.shape)\n",
    "            noise = np.random.normal(0, self.noise_std, blendshapes_in_video.shape)\n",
    "            blendshapes_in_video = blendshapes_in_video + noise\n",
    "            all_blendshapes.append(blendshapes_in_video)\n",
    "        # noise = np.random.normal(0, self.noise_std, data_blendshapes_lists.shape)\n",
    "        return all_blendshapes\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        mask = self.mask[idx]\n",
    "\n",
    "        if self.labels is not None:\n",
    "            sample_label = self.labels[idx]\n",
    "        else:\n",
    "            sample_label = None\n",
    "\n",
    "        return torch.tensor(sample, dtype=torch.float32), \\\n",
    "               torch.tensor(sample_label, dtype=torch.float32), \\\n",
    "               torch.tensor(mask, dtype=torch.float32)\n",
    "    \n",
    "\n",
    "def create_dataloader(all_data, batch_size=2, shuffle=True):\n",
    "    stacked_data = []  # 125 videos, F frames, 51 features\n",
    "    all_labels = []   # 125 videos, F frames\n",
    "    smoothed_velocities = []  # 125 videos, F frames\n",
    "    for video, video_frames in all_data.items():\n",
    "        data = []  # F, 51\n",
    "        labels = []  # F\n",
    "        smoothed_velocities_per_video = []  # F\n",
    "        for frame in video_frames:\n",
    "            label = frame.pop(\"occluded\")\n",
    "            smoothed_velocity = frame.pop(\"smoothed_velocity\")  # to create fake occlusions mostly where the changes are and not in the rest frames (window=5)\n",
    "            for k in [\"Timecode\", \"BlendshapeCount\", \"velocity\"]:\n",
    "                frame.pop(k, None)\n",
    "            data.append(frame)  # 51 features\n",
    "            labels.append(label)\n",
    "            smoothed_velocities_per_video.append(smoothed_velocity)\n",
    "        stacked_data.append(data)\n",
    "        all_labels.append(labels)\n",
    "        smoothed_velocities.append(smoothed_velocities_per_video)  # 125 videos, F frames\n",
    "\n",
    "    print(len(stacked_data[0]))\n",
    "    # print(stacked_data[0])\n",
    "    print(len(stacked_data))\n",
    "    print(len(all_labels))\n",
    "    dataset = TimeSeriesDataset(stacked_data, all_labels, smoothed_velocities)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'# read occlusions\\nwith np.load(\"occlusions_results_timecodes.npz\", allow_pickle=True) as data:\\n    video_occlusions = data[\"video_occlusions\"].item()\\nprint(len(video_occlusions))'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read occlusions\n",
    "with np.load(\"occlusions_results.npz\", allow_pickle=True) as data:\n",
    "    video_occlusions_frames = data[\"video_occlusions\"].item()\n",
    "print(len(video_occlusions_frames))\n",
    "\n",
    "\"\"\"# read occlusions\n",
    "with np.load(\"occlusions_results_timecodes.npz\", allow_pickle=True) as data:\n",
    "    video_occlusions = data[\"video_occlusions\"].item()\n",
    "print(len(video_occlusions))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read blendshapes from npz\n",
    "with np.load(\"blendshapes_timecodes_velocities_cubic_interpolated.npz\", allow_pickle=True) as data:\n",
    "    video_blendshapes_cubic = data[\"video_blendshapes\"].item()\n",
    "print(len(video_blendshapes_cubic))\n",
    "\n",
    "\n",
    "def add_occlusion_labels(video_blendshapes, occlusions):\n",
    "    for video_name, frames in video_blendshapes.items():\n",
    "        occlusion_frames = occlusions.get(video_name, [])  # get occlusions for video_name, if not found return empty list\n",
    "        occlusion_labels = np.zeros(len(frames), dtype=int)\n",
    "        \n",
    "        for start, end in occlusion_frames:\n",
    "            occlusion_labels[start:end+1] = 1\n",
    "        \n",
    "        for i, frame in enumerate(frames):\n",
    "            frame['occluded'] = occlusion_labels[i]\n",
    "    \n",
    "    return video_blendshapes\n",
    "\n",
    "# Add occlusion labels to video_blendshapes_cubic\n",
    "video_blendshapes_cubic = add_occlusion_labels(video_blendshapes_cubic, video_occlusions_frames)\n",
    "video_blendshapes_cubic[\"varg_002_2_pmil\"][0][\"occluded\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "354\n",
      "125\n",
      "125\n",
      "(354, 51)\n",
      "(824, 51)\n",
      "(387, 51)\n",
      "(1200, 51)\n",
      "(583, 51)\n",
      "(169, 51)\n",
      "(1046, 51)\n",
      "(1121, 51)\n",
      "(1065, 51)\n",
      "(778, 51)\n",
      "(1319, 51)\n",
      "(1169, 51)\n",
      "(636, 51)\n",
      "(521, 51)\n",
      "(479, 51)\n",
      "(381, 51)\n",
      "(1295, 51)\n",
      "(519, 51)\n",
      "(531, 51)\n",
      "(1358, 51)\n",
      "(480, 51)\n",
      "(456, 51)\n",
      "(937, 51)\n",
      "(437, 51)\n",
      "(2060, 51)\n",
      "(359, 51)\n",
      "(838, 51)\n",
      "(955, 51)\n",
      "(951, 51)\n",
      "(153, 51)\n",
      "(205, 51)\n",
      "(376, 51)\n",
      "(937, 51)\n",
      "(867, 51)\n",
      "(338, 51)\n",
      "(2118, 51)\n",
      "(1136, 51)\n",
      "(983, 51)\n",
      "(344, 51)\n",
      "(562, 51)\n",
      "(936, 51)\n",
      "(442, 51)\n",
      "(656, 51)\n",
      "(720, 51)\n",
      "(504, 51)\n",
      "(524, 51)\n",
      "(974, 51)\n",
      "(1282, 51)\n",
      "(410, 51)\n",
      "(563, 51)\n",
      "(938, 51)\n",
      "(942, 51)\n",
      "(747, 51)\n",
      "(438, 51)\n",
      "(1057, 51)\n",
      "(311, 51)\n",
      "(1191, 51)\n",
      "(328, 51)\n",
      "(1557, 51)\n",
      "ERROR: Number of blendshapes is not 51\n",
      "ERROR: Number of blendshapes is not 51\n",
      "ERROR: Number of blendshapes is not 51\n",
      "ERROR: Number of blendshapes is not 51\n",
      "ERROR: Number of blendshapes is not 51\n",
      "ERROR: Number of blendshapes is not 51\n",
      "ERROR: Number of blendshapes is not 51\n",
      "ERROR: Number of blendshapes is not 51\n",
      "ERROR: Number of blendshapes is not 51\n",
      "ERROR: Number of blendshapes is not 51\n",
      "ERROR: Number of blendshapes is not 51\n",
      "ERROR: Number of blendshapes is not 51\n",
      "ERROR: Number of blendshapes is not 51\n",
      "ERROR: Number of blendshapes is not 51\n",
      "ERROR: Number of blendshapes is not 51\n",
      "ERROR: Number of blendshapes is not 51\n",
      "ERROR: Number of blendshapes is not 51\n",
      "ERROR: Number of blendshapes is not 51\n",
      "ERROR: Number of blendshapes is not 51\n",
      "ERROR: Number of blendshapes is not 51\n",
      "ERROR: Number of blendshapes is not 51\n",
      "ERROR: Number of blendshapes is not 51\n",
      "ERROR: Number of blendshapes is not 51\n",
      "ERROR: Number of blendshapes is not 51\n",
      "ERROR: Number of blendshapes is not 51\n",
      "ERROR: Number of blendshapes is not 51\n",
      "ERROR: Number of blendshapes is not 51\n",
      "ERROR: Number of blendshapes is not 51\n",
      "ERROR: Number of blendshapes is not 51\n",
      "ERROR: Number of blendshapes is not 51\n",
      "ERROR: Number of blendshapes is not 51\n",
      "ERROR: Number of blendshapes is not 51\n",
      "ERROR: Number of blendshapes is not 51\n",
      "ERROR: Number of blendshapes is not 51\n",
      "ERROR: Number of blendshapes is not 51\n",
      "ERROR: Number of blendshapes is not 51\n",
      "ERROR: Number of blendshapes is not 51\n",
      "ERROR: Number of blendshapes is not 51\n",
      "ERROR: Number of blendshapes is not 51\n",
      "ERROR: Number of blendshapes is not 51\n",
      "ERROR: Number of blendshapes is not 51\n",
      "ERROR: Number of blendshapes is not 51\n",
      "ERROR: Number of blendshapes is not 51\n",
      "ERROR: Number of blendshapes is not 51\n",
      "ERROR: Number of blendshapes is not 51\n",
      "ERROR: Number of blendshapes is not 51\n",
      "ERROR: Number of blendshapes is not 51\n",
      "ERROR: Number of blendshapes is not 51\n",
      "ERROR: Number of blendshapes is not 51\n",
      "ERROR: Number of blendshapes is not 51\n",
      "ERROR: Number of blendshapes is not 51\n",
      "ERROR: Number of blendshapes is not 51\n",
      "ERROR: Number of blendshapes is not 51\n",
      "ERROR: Number of blendshapes is not 51\n",
      "ERROR: Number of blendshapes is not 51\n",
      "ERROR: Number of blendshapes is not 51\n",
      "ERROR: Number of blendshapes is not 51\n",
      "ERROR: Number of blendshapes is not 51\n",
      "ERROR: Number of blendshapes is not 51\n",
      "(62, 51)\n",
      "(344, 51)\n",
      "(1058, 51)\n",
      "(651, 51)\n",
      "(827, 51)\n",
      "(454, 51)\n",
      "(1078, 51)\n",
      "(2016, 51)\n",
      "(1054, 51)\n",
      "(726, 51)\n",
      "(1209, 51)\n",
      "(366, 51)\n",
      "(340, 51)\n",
      "(442, 51)\n",
      "(1228, 51)\n",
      "(1876, 51)\n",
      "(1078, 51)\n",
      "(451, 51)\n",
      "(662, 51)\n",
      "(1770, 51)\n",
      "(930, 51)\n",
      "(2096, 51)\n",
      "(426, 51)\n",
      "(231, 51)\n",
      "(367, 51)\n",
      "(350, 51)\n",
      "(1093, 51)\n",
      "(1194, 51)\n",
      "(351, 51)\n",
      "(1289, 51)\n",
      "(732, 51)\n",
      "(768, 51)\n",
      "(366, 51)\n",
      "(540, 51)\n",
      "(521, 51)\n",
      "(1018, 51)\n",
      "(1357, 51)\n",
      "(2488, 51)\n",
      "(681, 51)\n",
      "(592, 51)\n",
      "(1067, 51)\n",
      "(533, 51)\n",
      "(151, 51)\n",
      "(732, 51)\n",
      "(338, 51)\n",
      "(1027, 51)\n",
      "(359, 51)\n",
      "(1505, 51)\n",
      "(665, 51)\n",
      "(1282, 51)\n",
      "(452, 51)\n",
      "(1075, 51)\n",
      "(956, 51)\n",
      "(1148, 51)\n",
      "(404, 51)\n",
      "(539, 51)\n",
      "(1507, 51)\n",
      "(132, 51)\n",
      "(442, 51)\n",
      "(382, 51)\n",
      "(1179, 51)\n",
      "(447, 51)\n",
      "(897, 51)\n",
      "(987, 51)\n",
      "(491, 51)\n",
      "(2112, 51)\n",
      "\n",
      "Percentage of REAL occlusions: 0.039680040929573585\n",
      "\n",
      "Video 0 length: 354\n",
      "Video 1 length: 824\n",
      "Video 2 length: 387\n",
      "Video 3 length: 1200\n",
      "Video 4 length: 583\n",
      "Video 5 length: 169\n",
      "Video 6 length: 1046\n",
      "Video 7 length: 1121\n",
      "Video 8 length: 1065\n",
      "Video 9 length: 778\n",
      "Video 10 length: 1319\n",
      "Video 11 length: 1169\n",
      "Video 12 length: 636\n",
      "Video 13 length: 521\n",
      "Video 14 length: 479\n",
      "Video 15 length: 381\n",
      "Video 16 length: 1295\n",
      "Video 17 length: 519\n",
      "Video 18 length: 531\n",
      "Video 19 length: 1358\n",
      "Video 20 length: 480\n",
      "Video 21 length: 456\n",
      "Video 22 length: 937\n",
      "Video 23 length: 437\n",
      "Video 24 length: 2060\n",
      "Video 25 length: 359\n",
      "Video 26 length: 838\n",
      "Video 27 length: 955\n",
      "Video 28 length: 951\n",
      "Video 29 length: 153\n",
      "Video 30 length: 205\n",
      "Video 31 length: 376\n",
      "Video 32 length: 937\n",
      "Video 33 length: 867\n",
      "Video 34 length: 338\n",
      "Video 35 length: 2118\n",
      "Video 36 length: 1136\n",
      "Video 37 length: 983\n",
      "Video 38 length: 344\n",
      "Video 39 length: 562\n",
      "Video 40 length: 936\n",
      "Video 41 length: 442\n",
      "Video 42 length: 656\n",
      "Video 43 length: 720\n",
      "Video 44 length: 504\n",
      "Video 45 length: 524\n",
      "Video 46 length: 974\n",
      "Video 47 length: 1282\n",
      "Video 48 length: 410\n",
      "Video 49 length: 563\n",
      "Video 50 length: 938\n",
      "Video 51 length: 942\n",
      "Video 52 length: 747\n",
      "Video 53 length: 438\n",
      "Video 54 length: 1057\n",
      "Video 55 length: 311\n",
      "Video 56 length: 1191\n",
      "Video 57 length: 328\n",
      "Video 58 length: 1557\n",
      "Video 59 length: 62\n",
      "Video 60 length: 344\n",
      "Video 61 length: 1058\n",
      "Video 62 length: 651\n",
      "Video 63 length: 827\n",
      "Video 64 length: 454\n",
      "Video 65 length: 1078\n",
      "Video 66 length: 2016\n",
      "Video 67 length: 1054\n",
      "Video 68 length: 726\n",
      "Video 69 length: 1209\n",
      "Video 70 length: 366\n",
      "Video 71 length: 340\n",
      "Video 72 length: 442\n",
      "Video 73 length: 1228\n",
      "Video 74 length: 1876\n",
      "Video 75 length: 1078\n",
      "Video 76 length: 451\n",
      "Video 77 length: 662\n",
      "Video 78 length: 1770\n",
      "Video 79 length: 930\n",
      "Video 80 length: 2096\n",
      "Video 81 length: 426\n",
      "Video 82 length: 231\n",
      "Video 83 length: 367\n",
      "Video 84 length: 350\n",
      "Video 85 length: 1093\n",
      "Video 86 length: 1194\n",
      "Video 87 length: 351\n",
      "Video 88 length: 1289\n",
      "Video 89 length: 732\n",
      "Video 90 length: 768\n",
      "Video 91 length: 366\n",
      "Video 92 length: 540\n",
      "Video 93 length: 521\n",
      "Video 94 length: 1018\n",
      "Video 95 length: 1357\n",
      "Video 96 length: 2488\n",
      "Video 97 length: 681\n",
      "Video 98 length: 592\n",
      "Video 99 length: 1067\n",
      "Video 100 length: 533\n",
      "Video 101 length: 151\n",
      "Video 102 length: 732\n",
      "Video 103 length: 338\n",
      "Video 104 length: 1027\n",
      "Video 105 length: 359\n",
      "Video 106 length: 1505\n",
      "Video 107 length: 665\n",
      "Video 108 length: 1282\n",
      "Video 109 length: 452\n",
      "Video 110 length: 1075\n",
      "Video 111 length: 956\n",
      "Video 112 length: 1148\n",
      "Video 113 length: 404\n",
      "Video 114 length: 539\n",
      "Video 115 length: 1507\n",
      "Video 116 length: 132\n",
      "Video 117 length: 442\n",
      "Video 118 length: 382\n",
      "Video 119 length: 1179\n",
      "Video 120 length: 447\n",
      "Video 121 length: 897\n",
      "Video 122 length: 987\n",
      "Video 123 length: 491\n",
      "Video 124 length: 2112\n",
      "\n",
      "Percentage of occlusions: 0.21442767468860072\n",
      "\n",
      "\n",
      "Percentage of masked data: 0.17474763375902713\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_loader = create_dataloader(video_blendshapes_cubic, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "for batch in test_loader:\n",
    "    print(batch[0].shape)\n",
    "    print(batch[1].shape)\n",
    "    print(batch[2].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a test train val split\n",
    "video_blendshapes_cubic"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
