{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling\n",
    "\n",
    "* count real occlusions over the whole dataset\n",
    "\n",
    "```\n",
    "Average length of occlusions: 2.92 frames\n",
    "Average number of occluded frames per video: 24.15 frames\n",
    "Average percentage of occluded frames per video: 3.17%\n",
    "Total number of occluded frames: 2994 out of total for the dataset 101156 \n",
    "```\n",
    "\n",
    "* \"Target\" for real occlusions: \n",
    "\n",
    "1. random numbers\n",
    "2. linear interpolations\n",
    "3. --> cubic interpolations\n",
    "\n",
    "* ✅ add noise\n",
    "* ✅ create DataLoader object\n",
    "* scaling\n",
    "* ✅ define train-val-test split\n",
    "* ✅ create bi-LSTM object\n",
    "* ✅ create training loop\n",
    "* ✅ create testing loop\n",
    "* ✅ create predict function for one video\n",
    "* save sample .csv of blendshapes for all models (3 so far)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import ArtistAnimation\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import interpolate\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "# Standardization functions\n",
    "def fit_and_standardize(data):\n",
    "    shape = data.shape\n",
    "    flat = data.reshape((-1, data.shape[-1]))\n",
    "    scaler = StandardScaler().fit(flat)\n",
    "    scaled = scaler.transform(flat).reshape(shape)\n",
    "    return scaled, scaler\n",
    "\n",
    "def standardize(data, scaler):\n",
    "    shape = data.shape\n",
    "    flat = data.reshape((-1, data.shape[-1]))\n",
    "    scaled = scaler.transform(flat).reshape(shape)\n",
    "    return scaled\n",
    "\n",
    "\n",
    "def inv_standardize(data, scaler):\n",
    "    shape = data.shape\n",
    "    flat = data.reshape((-1, data.shape[-1]))\n",
    "    scaled = scaler.inverse_transform(flat).reshape(shape)\n",
    "    return scaled\n",
    "\n",
    "\n",
    "# Data processing function\n",
    "def process_data(data):\n",
    "    gaps = np.isnan(data)\n",
    "    all_null = np.sum(np.sum(gaps, axis=1), axis=1) > gaps.shape[1] * gaps.shape[2] * 0.8\n",
    "    data = data[~all_null, :, :]\n",
    "    gaps = gaps[~all_null, :, :]\n",
    "    keep = ~np.isnan(np.sum(data, axis=(1, 2)))\n",
    "    return data[keep, :, :], gaps[keep, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read blendshapes from npz\n",
    "with np.load(\"blendshapes_timecodes_velocities_cubic_interpolated.npz\", allow_pickle=True) as data:\n",
    "    video_blendshapes_cubic = data[\"video_blendshapes\"].item()\n",
    "print(len(video_blendshapes_cubic))\n",
    "\n",
    "# Define the rest face blendshapes\n",
    "calibration_path = \"./20241101_face-calib_002_2\"\n",
    "rest_face_blendshapes = {k:v for k, v in video_blendshapes_cubic[\"face-calib_002_2_pmil\"][5].items()} # if k not in [\"Timecode\", \"BlendshapeCount\", \"velocity\", \"smoothed_velocity\"]}\n",
    "\n",
    "smoothed_velocity_rest_face = rest_face_blendshapes.pop(\"smoothed_velocity\")\n",
    "for k in [\"Timecode\", \"BlendshapeCount\", \"velocity\"]:\n",
    "    rest_face_blendshapes.pop(k, None)\n",
    "rest_face_frame_as_list = list(rest_face_blendshapes.values())   # F frames, 51 features\n",
    "len(rest_face_frame_as_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, data, labels=None, smoothed_velocities=None, occ_prob=0.3, noise_std=0.01):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data (np.array): Ground truth data of shape (num_videos, num_frames, blendshapes_dim). 125 videos, F frames, 51 features\n",
    "            labels (np.array): Binary occlusion labels of shape (num_videos, num_frames).  (125, F)\n",
    "            smoothed_velocities (np.array): Smoothed velocity for each frame of the video (num_videos, num_frames).  (125, F)\n",
    "            occ_prob (float): Probability of adding synthetic occlusions.\n",
    "            noise_std (float): Standard deviation of Gaussian noise to add to data.\n",
    "        \"\"\"\n",
    "        self.data = data.copy()\n",
    "        self.labels = labels.copy()\n",
    "        self.smoothed_velocities = smoothed_velocities.copy()\n",
    "        self.occ_prob = occ_prob\n",
    "        self.noise_std = noise_std\n",
    "\n",
    "        # Add Gaussian noise to the entire dataset\n",
    "        self.data = self.add_noise(self.data)\n",
    "\n",
    "        self.data, self.labels, self.mask = self.insert_fake_occlussions() \n",
    "\n",
    "        # Sort data over second dimension for bucketing\n",
    "        # print(\"len(self.data): \", len(self.data))\n",
    "        # self.data = [sorted(video, key=lambda x: len(x), reverse=True) for video in self.data]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def insert_fake_occlussions(self):\n",
    "        data = self.data.copy()  # (125 videos, F frames, 51 features)\n",
    "        labels = self.labels.copy()\n",
    "        # Create one hot vector that says which data is real 1 and which data is fake 0\n",
    "        # mask = np.ones_like(labels, dtype=int)  # 1 for real, 0 for synthetic\n",
    "        mask = [[1] * len(video) for video in data]\n",
    "\n",
    "        total_frames = sum([len(video) for video in data])\n",
    "        occ_p_init = sum([sum(video) for video in labels]) / total_frames\n",
    "        print(f\"\\nPercentage of REAL occlusions: {occ_p_init}\\n\")\n",
    "        \n",
    "        for video_i, video in enumerate(data):\n",
    "            len_video = len(video)\n",
    "            average_velocity = np.mean(self.smoothed_velocities[video_i])\n",
    "            std_velocity = np.std(self.smoothed_velocities[video_i])\n",
    "            print(f\"Video {video_i} length: {len_video}\")\n",
    "            for frame_n, frame in enumerate(video):\n",
    "\n",
    "                # if the current frame velocity is higher than the average + 1 std, then add an occlusion with a probability\n",
    "                if self.smoothed_velocities[video_i][frame_n] > average_velocity + std_velocity:\n",
    "                    \n",
    "                    if np.random.rand() < self.occ_prob:\n",
    "                        occ_len = int(np.clip(np.random.normal(2.92, 12.05), 2, 24))   # mean and std from the dataset\n",
    "                        # print(f\"Occ len: {occ_len}\")\n",
    "                        # start = np.random.randint(0, len_video - occ_len)\n",
    "                        start = frame_n\n",
    "                        end = start + occ_len\n",
    "                        if end + 1 >= len_video:\n",
    "                            end = len_video - 2\n",
    "                        # check if the occlusion is not already there and update mask only where there wasn't an occlusion already\n",
    "                        mask_update_indices = (np.array(labels[video_i][start:end+1]) != 1)\n",
    "                        # print(f\"Mask update indices: {mask_update_indices}\")\n",
    "                        mask[video_i][start:end+1] = [0 if mask_update_indices[k] else el for k, el in enumerate(mask[video_i][start:end+1])]  # update mask with zeros where fake occlusions are\n",
    "                        labels[video_i][start:end+1] = [1 if mask_update_indices[k] else el for k, el in enumerate(labels[video_i][start:end+1])] # update labels to have 1 for occlusions\n",
    "                        # do nothing to the `data` as it stands for ground truth\n",
    "\n",
    "        # total percentage of all occlusions after inserting fake ones \n",
    "        occ_p = sum([sum(video) for video in labels]) / total_frames\n",
    "        print(f\"\\nPercentage of occlusions: {occ_p}\\n\")\n",
    "        # percentage of masked data\n",
    "        mask_p = 1 - (sum([sum(video) for video in mask]) / total_frames)\n",
    "        print(f\"\\nPercentage of masked data: {mask_p}\\n\")\n",
    "        return data, labels, mask\n",
    "    \n",
    "    def add_noise(self, data):\n",
    "        # Add Gaussian noise across all frames in each video\n",
    "        all_blendshapes = []\n",
    "        for video in data:\n",
    "            blendshapes_in_video = []\n",
    "            for frame in video:\n",
    "                # order the keys in dict `frame` alphabetically\n",
    "                frame = dict(sorted(frame.items()))\n",
    "                blendshapes = frame.values()  # 51 features\n",
    "                if len(blendshapes) != 51:\n",
    "                    print(\"ERROR: Number of blendshapes is not 51\")\n",
    "                    blendshapes = [0.0] * 51\n",
    "                blendshapes = np.array(list(blendshapes))\n",
    "                blendshapes_in_video.append(blendshapes)\n",
    "            # Apply noise across all frames in the video\n",
    "            blendshapes_in_video = np.array(blendshapes_in_video)  # F, 51\n",
    "            print(blendshapes_in_video.shape)\n",
    "            noise = np.random.normal(0, self.noise_std, blendshapes_in_video.shape)\n",
    "            blendshapes_in_video = blendshapes_in_video + noise\n",
    "            all_blendshapes.append(blendshapes_in_video)\n",
    "        # noise = np.random.normal(0, self.noise_std, data_blendshapes_lists.shape)\n",
    "        return all_blendshapes\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        mask = self.mask[idx]\n",
    "\n",
    "        if self.labels is not None:\n",
    "            sample_label = self.labels[idx]\n",
    "        else:\n",
    "            sample_label = None\n",
    "\n",
    "        return torch.tensor(np.array(sample), dtype=torch.float32), \\\n",
    "               torch.tensor(np.array(sample_label), dtype=torch.float32), \\\n",
    "               torch.tensor(np.array(mask), dtype=torch.float32)\n",
    "        \n",
    "\n",
    "def collate_fn(batch):\n",
    "    data = [item[0] for item in batch]\n",
    "    labels = [item[1] for item in batch]\n",
    "    mask = [item[2] for item in batch]\n",
    "\n",
    "    sequence_lengths = torch.tensor([len(seq) for seq in data])\n",
    "\n",
    "    # padding the sorted sequences and keeping their lengths\n",
    "    data = nn.utils.rnn.pad_sequence(data, batch_first=True, padding_value=0.0)  # rest_face_frame_as_list)\n",
    "    labels = nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=0)\n",
    "    mask = nn.utils.rnn.pad_sequence(mask, batch_first=True, padding_value=1)\n",
    "    return data, labels, mask, sequence_lengths\n",
    "\n",
    "\n",
    "def create_dataloader(all_data, batch_size=8, shuffle=True):\n",
    "    stacked_data = []  # 125 videos, F frames, 51 features\n",
    "    all_labels = []   # 125 videos, F frames\n",
    "    smoothed_velocities = []  # 125 videos, F frames\n",
    "    for video, video_frames in all_data.items():\n",
    "        data = []  # F, 51\n",
    "        labels = []  # F\n",
    "        smoothed_velocities_per_video = []  # F\n",
    "        for frame in video_frames:\n",
    "            label = frame.pop(\"occluded\")\n",
    "            smoothed_velocity = frame.pop(\"smoothed_velocity\")  # to create fake occlusions mostly where the changes are and not in the rest frames (window=5)\n",
    "            for k in [\"Timecode\", \"BlendshapeCount\", \"velocity\"]:\n",
    "                frame.pop(k, None)\n",
    "            data.append(frame)  # 51 features\n",
    "            labels.append(label)\n",
    "            smoothed_velocities_per_video.append(smoothed_velocity)\n",
    "        stacked_data.append(data)\n",
    "        all_labels.append(labels)\n",
    "        smoothed_velocities.append(smoothed_velocities_per_video)  # 125 videos, F frames\n",
    "\n",
    "        print(f\"Video {video} length: {len(video_frames)}\")\n",
    "\n",
    "    print(len(stacked_data[0]))\n",
    "    # print(stacked_data[0])\n",
    "    print(len(stacked_data))\n",
    "    print(len(all_labels))\n",
    "    dataset = TimeSeriesDataset(stacked_data, all_labels, smoothed_velocities)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125\n",
      "[(151, 153), (320, 321), (534, 542), (553, 560), (561, 562), (564, 566), (581, 593), (594, 594), (595, 595), (596, 596), (597, 600), (601, 605), (613, 614), (616, 618), (625, 627), (652, 664), (824, 829), (830, 834), (841, 845), (865, 879), (880, 883), (884, 885), (929, 930), (932, 933), (934, 942), (943, 946)]\n",
      "1054\n"
     ]
    }
   ],
   "source": [
    "# read occlusions\n",
    "with np.load(\"occlusions_results.npz\", allow_pickle=True) as data:\n",
    "    video_occlusions_frames = data[\"video_occlusions\"].item()\n",
    "print(len(video_occlusions_frames))\n",
    "\n",
    "\"\"\"# read occlusions\n",
    "with np.load(\"occlusions_results_timecodes.npz\", allow_pickle=True) as data:\n",
    "    video_occlusions = data[\"video_occlusions\"].item()\n",
    "print(len(video_occlusions))\"\"\"\n",
    "\n",
    "print(video_occlusions_frames['varg_002_2_pmil'])\n",
    "\n",
    "print(len(video_blendshapes_cubic['varg_002_2_pmil']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Train data: 87 videos, Test data: 19 videos, Validation data: 19 videos\n"
     ]
    }
   ],
   "source": [
    "def add_occlusion_labels(video_blendshapes, occlusions):\n",
    "    for video_name, frames in video_blendshapes.items():\n",
    "        occlusion_frames = occlusions.get(video_name, [])  # get occlusions for video_name, if not found return empty list\n",
    "        occlusion_labels = np.zeros(len(frames), dtype=int)\n",
    "        \n",
    "        for start, end in occlusion_frames:\n",
    "            occlusion_labels[start:end+1] = 1\n",
    "        \n",
    "        for i, frame in enumerate(frames):\n",
    "            frame['occluded'] = occlusion_labels[i]\n",
    "    \n",
    "    return video_blendshapes\n",
    "\n",
    "\n",
    "# train test val split of original npz before creating fake occlusions and before packing it into dataloader\n",
    "# TODO: separate out real occlusions into test and val, train only on fake occlusions\n",
    "def train_test_val_split(data, train_p=0.7, test_p=0.15, val_p=0.15):\n",
    "    # Split by number of videos\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        data (dict of dicts): List of dictionaries where each dictionary represents a video. 125 videos, 51 features for each frame in each video \n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    data_values, video_names = list(data.values()), list(data.keys())\n",
    "    np.random.shuffle(data_values)\n",
    "\n",
    "    train_data = data_values[:int(len(data_values) * train_p)]\n",
    "    test_data = data_values[int(len(data_values) * train_p):int(len(data_values) * (train_p + test_p))]\n",
    "    val_data = data_values[int(len(data_values) * (train_p + test_p)):]\n",
    "\n",
    "    print(f\"Train data: {len(train_data)} videos, Test data: {len(test_data)} videos, Validation data: {len(val_data)} videos\")\n",
    "\n",
    "    # turn the list of dictionaries back into a dictionary of dictionaries\n",
    "    train_data = {video_names[i]: train_data[i] for i in range(len(train_data))}\n",
    "    test_data = {video_names[i]: test_data[i] for i in range(len(test_data))}\n",
    "    val_data = {video_names[i]: val_data[i] for i in range(len(val_data))}\n",
    "\n",
    "    return train_data, test_data, val_data\n",
    "\n",
    "\n",
    "# Add occlusion labels to video_blendshapes_cubic\n",
    "video_blendshapes_cubic = add_occlusion_labels(video_blendshapes_cubic, video_occlusions_frames)\n",
    "print(video_blendshapes_cubic[\"varg_002_2_pmil\"][0][\"occluded\"])\n",
    "\n",
    "# Split the data into train, test, and validation sets\n",
    "train_data, test_data, val_data = train_test_val_split(video_blendshapes_cubic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video static_ROOF_fingers_contact_at_45_degrees_neutral_location_001_1_pmil length: 592\n",
      "Video varg_001_1_pmil length: 442\n",
      "Video bi_005_5_pmil length: 1557\n",
      "Video fran_001_1_pmil length: 768\n",
      "Video parti-vill-kalla-grupp-for-terrorister-1-det_001_1_pmil length: 410\n",
      "Video parti-vill-kalla-grupp-for-terrorister-4-nu_002_2_pmil length: 1289\n",
      "Video katt_001_1_pmil length: 447\n",
      "Video bok_002_2_pmil length: 328\n",
      "Video varg_003_3_pmil length: 1078\n",
      "Video parti-vill-kalla-grupp-for-terrorister-1-det_003_3_pmil length: 937\n",
      "Video djur_001_1_pmil length: 442\n",
      "Video abborre_002_2_pmil length: 62\n",
      "Video parti-vill-kalla-grupp-for-terrorister-5-bosattarna_003_3_pmil length: 827\n",
      "Video parti-vill-kalla-grupp-for-terrorister-7-det_001_1_pmil length: 1027\n",
      "Video hundvalp_001_1_pmil length: 983\n",
      "Video left_hand_on_top_of_the_battery_neutral_location_001_1_pmil length: 153\n",
      "Video parti-vill-kalla-grupp-for-terrorister-5-bosattarna_001_1_pmil length: 956\n",
      "Video touch_chin_with_index_finger_side_of_the_chin_far_from_active_hand_active_hand_001_1_pmil length: 824\n",
      "Video ogon_cheeks_active_hand_in_each_place_001_1_pmil length: 747\n",
      "592\n",
      "19\n",
      "19\n",
      "(592, 51)\n",
      "(442, 51)\n",
      "(1557, 51)\n",
      "(768, 51)\n",
      "(410, 51)\n",
      "(1289, 51)\n",
      "(447, 51)\n",
      "(328, 51)\n",
      "(1078, 51)\n",
      "(937, 51)\n",
      "(442, 51)\n",
      "ERROR: Number of blendshapes is not 51\n",
      "ERROR: Number of blendshapes is not 51\n",
      "ERROR: Number of blendshapes is not 51\n",
      "ERROR: Number of blendshapes is not 51\n",
      "ERROR: Number of blendshapes is not 51\n",
      "ERROR: Number of blendshapes is not 51\n",
      "ERROR: Number of blendshapes is not 51\n",
      "ERROR: Number of blendshapes is not 51\n",
      "ERROR: Number of blendshapes is not 51\n",
      "ERROR: Number of blendshapes is not 51\n",
      "ERROR: Number of blendshapes is not 51\n",
      "ERROR: Number of blendshapes is not 51\n",
      "ERROR: Number of blendshapes is not 51\n",
      "ERROR: Number of blendshapes is not 51\n",
      "ERROR: Number of blendshapes is not 51\n",
      "ERROR: Number of blendshapes is not 51\n",
      "ERROR: Number of blendshapes is not 51\n",
      "ERROR: Number of blendshapes is not 51\n",
      "ERROR: Number of blendshapes is not 51\n",
      "ERROR: Number of blendshapes is not 51\n",
      "ERROR: Number of blendshapes is not 51\n",
      "ERROR: Number of blendshapes is not 51\n",
      "ERROR: Number of blendshapes is not 51\n",
      "ERROR: Number of blendshapes is not 51\n",
      "ERROR: Number of blendshapes is not 51\n",
      "ERROR: Number of blendshapes is not 51\n",
      "ERROR: Number of blendshapes is not 51\n",
      "ERROR: Number of blendshapes is not 51\n",
      "ERROR: Number of blendshapes is not 51\n",
      "ERROR: Number of blendshapes is not 51\n",
      "ERROR: Number of blendshapes is not 51\n",
      "ERROR: Number of blendshapes is not 51\n",
      "ERROR: Number of blendshapes is not 51\n",
      "ERROR: Number of blendshapes is not 51\n",
      "ERROR: Number of blendshapes is not 51\n",
      "ERROR: Number of blendshapes is not 51\n",
      "ERROR: Number of blendshapes is not 51\n",
      "ERROR: Number of blendshapes is not 51\n",
      "ERROR: Number of blendshapes is not 51\n",
      "ERROR: Number of blendshapes is not 51\n",
      "ERROR: Number of blendshapes is not 51\n",
      "ERROR: Number of blendshapes is not 51\n",
      "ERROR: Number of blendshapes is not 51\n",
      "ERROR: Number of blendshapes is not 51\n",
      "ERROR: Number of blendshapes is not 51\n",
      "ERROR: Number of blendshapes is not 51\n",
      "ERROR: Number of blendshapes is not 51\n",
      "ERROR: Number of blendshapes is not 51\n",
      "ERROR: Number of blendshapes is not 51\n",
      "ERROR: Number of blendshapes is not 51\n",
      "ERROR: Number of blendshapes is not 51\n",
      "ERROR: Number of blendshapes is not 51\n",
      "ERROR: Number of blendshapes is not 51\n",
      "ERROR: Number of blendshapes is not 51\n",
      "ERROR: Number of blendshapes is not 51\n",
      "ERROR: Number of blendshapes is not 51\n",
      "ERROR: Number of blendshapes is not 51\n",
      "ERROR: Number of blendshapes is not 51\n",
      "ERROR: Number of blendshapes is not 51\n",
      "(62, 51)\n",
      "(827, 51)\n",
      "(1027, 51)\n",
      "(983, 51)\n",
      "(153, 51)\n",
      "(956, 51)\n",
      "(824, 51)\n",
      "(747, 51)\n",
      "\n",
      "Percentage of REAL occlusions: 0.03533059340976278\n",
      "\n",
      "Video 0 length: 592\n",
      "Video 1 length: 442\n",
      "Video 2 length: 1557\n",
      "Video 3 length: 768\n",
      "Video 4 length: 410\n",
      "Video 5 length: 1289\n",
      "Video 6 length: 447\n",
      "Video 7 length: 328\n",
      "Video 8 length: 1078\n",
      "Video 9 length: 937\n",
      "Video 10 length: 442\n",
      "Video 11 length: 62\n",
      "Video 12 length: 827\n",
      "Video 13 length: 1027\n",
      "Video 14 length: 983\n",
      "Video 15 length: 153\n",
      "Video 16 length: 956\n",
      "Video 17 length: 824\n",
      "Video 18 length: 747\n",
      "\n",
      "Percentage of occlusions: 0.2047732352729108\n",
      "\n",
      "\n",
      "Percentage of masked data: 0.16944264186314806\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_loader = create_dataloader(test_data, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 1557, 51])\n",
      "torch.Size([8, 1557])\n",
      "torch.Size([8, 1557])\n",
      "torch.Size([8])\n",
      "tensor([ 592,  442, 1557,  768,  410, 1289,  447,  328])\n",
      "torch.Size([8, 1078, 51])\n",
      "torch.Size([8, 1078])\n",
      "torch.Size([8, 1078])\n",
      "torch.Size([8])\n",
      "tensor([1078,  937,  442,   62,  827, 1027,  983,  153])\n",
      "torch.Size([3, 956, 51])\n",
      "torch.Size([3, 956])\n",
      "torch.Size([3, 956])\n",
      "torch.Size([3])\n",
      "tensor([956, 824, 747])\n"
     ]
    }
   ],
   "source": [
    "for batch in test_loader:\n",
    "    print(batch[0].shape)  # sample\n",
    "    print(batch[1].shape)  # sample_label\n",
    "    print(batch[2].shape)  # mask\n",
    "    print(batch[3].shape)\n",
    "    print(batch[3])  # sequence lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video static_ROOF_fingers_contact_at_45_degrees_neutral_location_001_1_pmil length: 531\n",
      "Video varg_001_1_pmil length: 656\n",
      "Video bi_005_5_pmil length: 1136\n",
      "Video fran_001_1_pmil length: 451\n",
      "Video parti-vill-kalla-grupp-for-terrorister-1-det_001_1_pmil length: 438\n",
      "Video parti-vill-kalla-grupp-for-terrorister-4-nu_002_2_pmil length: 366\n",
      "Video katt_001_1_pmil length: 1191\n",
      "Video bok_002_2_pmil length: 491\n",
      "Video varg_003_3_pmil length: 521\n",
      "Video parti-vill-kalla-grupp-for-terrorister-1-det_003_3_pmil length: 583\n",
      "Video djur_001_1_pmil length: 955\n",
      "Video abborre_002_2_pmil length: 132\n",
      "Video parti-vill-kalla-grupp-for-terrorister-5-bosattarna_003_3_pmil length: 504\n",
      "Video parti-vill-kalla-grupp-for-terrorister-7-det_001_1_pmil length: 2488\n",
      "Video hundvalp_001_1_pmil length: 1358\n",
      "Video left_hand_on_top_of_the_battery_neutral_location_001_1_pmil length: 662\n",
      "Video parti-vill-kalla-grupp-for-terrorister-5-bosattarna_001_1_pmil length: 1319\n",
      "Video touch_chin_with_index_finger_side_of_the_chin_far_from_active_hand_active_hand_001_1_pmil length: 151\n",
      "Video ogon_cheeks_active_hand_in_each_place_001_1_pmil length: 1169\n",
      "Video mussla_002_2_pmil length: 539\n",
      "Video right_hand_above_belly_button_001_1_pmil length: 1282\n",
      "Video static_ROOF_fingers_contact_at_45_degrees_left_top_location_001_1_pmil length: 1770\n",
      "Video skara_001_1_pmil length: 354\n",
      "Video minister-flydde-fran-tomater-story_001_1_pmil length: 838\n",
      "Video hundvalp_003_3_pmil length: 376\n",
      "Video static_closed_book_left_bottom_location_001_1_pmil length: 1054\n",
      "Video stada_001_1_pmil length: 636\n",
      "Video tro_001_1_pmil length: 2060\n",
      "Video ensam_002_2_pmil length: 524\n",
      "Video baver_002_2_pmil length: 1282\n",
      "Video parti-vill-kalla-grupp-for-terrorister-2-men_002_2_pmil length: 732\n",
      "Video static_closed_book_right_top_location_001_1_pmil length: 1228\n",
      "Video till_001_1_pmil length: 454\n",
      "Video sko_001_1_pmil length: 381\n",
      "Video right_hand_on_top_of_the_battery_neutral_location_001_1_pmil length: 1148\n",
      "Video halsband_002_2_pmil length: 937\n",
      "Video groda_002_2_pmil length: 936\n",
      "Video kalkon_002_2_pmil length: 426\n",
      "Video static_ROOF_fingers_contact_at_45_degrees_right_top_location_001_1_pmil length: 1093\n",
      "Video tip_of_the_nose_touch_and_hold_active_hand_001_1_pmil length: 205\n",
      "Video min_001_1_pmil length: 1018\n",
      "Video touch_chin_with_index_finger_middle_of_the_chin_active_hand_001_1_pmil length: 311\n",
      "Video nytt-forslag-om-invandrare-1-regeringen_002_2_pmil length: 778\n",
      "Video bi_002_2_pmil length: 867\n",
      "Video nytt-forslag-om-invandrare-3-nu_001_1_pmil length: 651\n",
      "Video forehead-to-chin_NOT_UNDERSTAND_R_chin_forehead_active_hand_in_each_place_001_1_pmil length: 359\n",
      "Video glad_002_2_pmil length: 1179\n",
      "Video kobra_002_2_pmil length: 951\n",
      "Video static_JOBBA_left_bottom_location_001_1_pmil length: 2016\n",
      "Video parti-vill-kalla-grupp-for-terrorister-6-partiet_001_1_pmil length: 987\n",
      "Video fjaril_001_1_pmil length: 169\n",
      "Video hackspett_002_2_pmil length: 1078\n",
      "Video parti-vill-kalla-grupp-for-terrorister-3-en_001_1_pmil length: 366\n",
      "Video nytt-forslag-om-invandrare-4-politikerna_001_1_pmil length: 562\n",
      "Video matt_001_1_pmil length: 563\n",
      "Video static_closed_book_left_top_location_001_1_pmil length: 2118\n",
      "Video fasting_001_1_pmil length: 1295\n",
      "Video static_JOBBA_neutral_location_001_1_pmil length: 726\n",
      "Video face-calib_002_2_pmil length: 338\n",
      "Video static_ROOF_fingers_contact_at_45_degrees_in_front_of_the_face_location_001_1_pmil length: 1507\n",
      "Video static_JOBBA_left_top_location_001_1_pmil length: 1121\n",
      "Video parti-vill-kalla-grupp-for-terrorister-5-bosattarna_004_4_pmil length: 382\n",
      "Video t-pose_001_1_pmil length: 720\n",
      "Video ensam_001_1_pmil length: 1357\n",
      "Video right_hand_resting_on_the_chest_chest_001_1_pmil length: 1209\n",
      "Video fagel_001_1_pmil length: 533\n",
      "Video hand_alphabet_001_1_pmil length: 359\n",
      "Video varg_002_2_pmil length: 1065\n",
      "Video parti-vill-kalla-grupp-for-terrorister-2-men_001_1_pmil length: 521\n",
      "Video baver_001_1_pmil length: 452\n",
      "Video groda_003_3_pmil length: 1200\n",
      "Video static_JOBBA_right_bottom_location_001_1_pmil length: 519\n",
      "Video left_hand_resting_on_the_chest_chest_001_1_pmil length: 344\n",
      "Video bi_003_3_pmil length: 442\n",
      "Video apa_001_1_pmil length: 681\n",
      "Video anka_001_1_pmil length: 1046\n",
      "Video static_JOBBA_right_top_location_001_1_pmil length: 2096\n",
      "Video parti-vill-kalla-grupp-for-terrorister-2-men_003_3_pmil length: 665\n",
      "Video baver_003_3_pmil length: 1075\n",
      "Video halsband_001_1_pmil length: 1057\n",
      "Video groda_001_1_pmil length: 938\n",
      "Video kalkon_001_1_pmil length: 367\n",
      "Video bi_001_1_pmil length: 974\n",
      "Video nytt-forslag-om-invandrare-1-regeringen_001_1_pmil length: 350\n",
      "Video static_ROOF_fingers_contact_at_45_degrees_right_bottom_location_001_1_pmil length: 1058\n",
      "Video bock_001_1_pmil length: 897\n",
      "Video kobra_001_1_pmil length: 930\n",
      "531\n",
      "87\n",
      "87\n",
      "(531, 51)\n",
      "(656, 51)\n",
      "(1136, 51)\n",
      "(451, 51)\n",
      "(438, 51)\n",
      "(366, 51)\n",
      "(1191, 51)\n",
      "(491, 51)\n",
      "(521, 51)\n",
      "(583, 51)\n",
      "(955, 51)\n",
      "(132, 51)\n",
      "(504, 51)\n",
      "(2488, 51)\n",
      "(1358, 51)\n",
      "(662, 51)\n",
      "(1319, 51)\n",
      "(151, 51)\n",
      "(1169, 51)\n",
      "(539, 51)\n",
      "(1282, 51)\n",
      "(1770, 51)\n",
      "(354, 51)\n",
      "(838, 51)\n",
      "(376, 51)\n",
      "(1054, 51)\n",
      "(636, 51)\n",
      "(2060, 51)\n",
      "(524, 51)\n",
      "(1282, 51)\n",
      "(732, 51)\n",
      "(1228, 51)\n",
      "(454, 51)\n",
      "(381, 51)\n",
      "(1148, 51)\n",
      "(937, 51)\n",
      "(936, 51)\n",
      "(426, 51)\n",
      "(1093, 51)\n",
      "(205, 51)\n",
      "(1018, 51)\n",
      "(311, 51)\n",
      "(778, 51)\n",
      "(867, 51)\n",
      "(651, 51)\n",
      "(359, 51)\n",
      "(1179, 51)\n",
      "(951, 51)\n",
      "(2016, 51)\n",
      "(987, 51)\n",
      "(169, 51)\n",
      "(1078, 51)\n",
      "(366, 51)\n",
      "(562, 51)\n",
      "(563, 51)\n",
      "(2118, 51)\n",
      "(1295, 51)\n",
      "(726, 51)\n",
      "(338, 51)\n",
      "(1507, 51)\n",
      "(1121, 51)\n",
      "(382, 51)\n",
      "(720, 51)\n",
      "(1357, 51)\n",
      "(1209, 51)\n",
      "(533, 51)\n",
      "(359, 51)\n",
      "(1065, 51)\n",
      "(521, 51)\n",
      "(452, 51)\n",
      "(1200, 51)\n",
      "(519, 51)\n",
      "(344, 51)\n",
      "(442, 51)\n",
      "(681, 51)\n",
      "(1046, 51)\n",
      "(2096, 51)\n",
      "(665, 51)\n",
      "(1075, 51)\n",
      "(1057, 51)\n",
      "(938, 51)\n",
      "(367, 51)\n",
      "(974, 51)\n",
      "(350, 51)\n",
      "(1058, 51)\n",
      "(897, 51)\n",
      "(930, 51)\n",
      "\n",
      "Percentage of REAL occlusions: 0.04062321559670446\n",
      "\n",
      "Video 0 length: 531\n",
      "Video 1 length: 656\n",
      "Video 2 length: 1136\n",
      "Video 3 length: 451\n",
      "Video 4 length: 438\n",
      "Video 5 length: 366\n",
      "Video 6 length: 1191\n",
      "Video 7 length: 491\n",
      "Video 8 length: 521\n",
      "Video 9 length: 583\n",
      "Video 10 length: 955\n",
      "Video 11 length: 132\n",
      "Video 12 length: 504\n",
      "Video 13 length: 2488\n",
      "Video 14 length: 1358\n",
      "Video 15 length: 662\n",
      "Video 16 length: 1319\n",
      "Video 17 length: 151\n",
      "Video 18 length: 1169\n",
      "Video 19 length: 539\n",
      "Video 20 length: 1282\n",
      "Video 21 length: 1770\n",
      "Video 22 length: 354\n",
      "Video 23 length: 838\n",
      "Video 24 length: 376\n",
      "Video 25 length: 1054\n",
      "Video 26 length: 636\n",
      "Video 27 length: 2060\n",
      "Video 28 length: 524\n",
      "Video 29 length: 1282\n",
      "Video 30 length: 732\n",
      "Video 31 length: 1228\n",
      "Video 32 length: 454\n",
      "Video 33 length: 381\n",
      "Video 34 length: 1148\n",
      "Video 35 length: 937\n",
      "Video 36 length: 936\n",
      "Video 37 length: 426\n",
      "Video 38 length: 1093\n",
      "Video 39 length: 205\n",
      "Video 40 length: 1018\n",
      "Video 41 length: 311\n",
      "Video 42 length: 778\n",
      "Video 43 length: 867\n",
      "Video 44 length: 651\n",
      "Video 45 length: 359\n",
      "Video 46 length: 1179\n",
      "Video 47 length: 951\n",
      "Video 48 length: 2016\n",
      "Video 49 length: 987\n",
      "Video 50 length: 169\n",
      "Video 51 length: 1078\n",
      "Video 52 length: 366\n",
      "Video 53 length: 562\n",
      "Video 54 length: 563\n",
      "Video 55 length: 2118\n",
      "Video 56 length: 1295\n",
      "Video 57 length: 726\n",
      "Video 58 length: 338\n",
      "Video 59 length: 1507\n",
      "Video 60 length: 1121\n",
      "Video 61 length: 382\n",
      "Video 62 length: 720\n",
      "Video 63 length: 1357\n",
      "Video 64 length: 1209\n",
      "Video 65 length: 533\n",
      "Video 66 length: 359\n",
      "Video 67 length: 1065\n",
      "Video 68 length: 521\n",
      "Video 69 length: 452\n",
      "Video 70 length: 1200\n",
      "Video 71 length: 519\n",
      "Video 72 length: 344\n",
      "Video 73 length: 442\n",
      "Video 74 length: 681\n",
      "Video 75 length: 1046\n",
      "Video 76 length: 2096\n",
      "Video 77 length: 665\n",
      "Video 78 length: 1075\n",
      "Video 79 length: 1057\n",
      "Video 80 length: 938\n",
      "Video 81 length: 367\n",
      "Video 82 length: 974\n",
      "Video 83 length: 350\n",
      "Video 84 length: 1058\n",
      "Video 85 length: 897\n",
      "Video 86 length: 930\n",
      "\n",
      "Percentage of occlusions: 0.21414199091823694\n",
      "\n",
      "\n",
      "Percentage of masked data: 0.17351877532153248\n",
      "\n",
      "Video static_ROOF_fingers_contact_at_45_degrees_neutral_location_001_1_pmil length: 456\n",
      "Video varg_001_1_pmil length: 387\n",
      "Video bi_005_5_pmil length: 437\n",
      "Video fran_001_1_pmil length: 338\n",
      "Video parti-vill-kalla-grupp-for-terrorister-1-det_001_1_pmil length: 1067\n",
      "Video parti-vill-kalla-grupp-for-terrorister-4-nu_002_2_pmil length: 351\n",
      "Video katt_001_1_pmil length: 404\n",
      "Video bok_002_2_pmil length: 1876\n",
      "Video varg_003_3_pmil length: 1194\n",
      "Video parti-vill-kalla-grupp-for-terrorister-1-det_003_3_pmil length: 231\n",
      "Video djur_001_1_pmil length: 2112\n",
      "Video abborre_002_2_pmil length: 480\n",
      "Video parti-vill-kalla-grupp-for-terrorister-5-bosattarna_003_3_pmil length: 344\n",
      "Video parti-vill-kalla-grupp-for-terrorister-7-det_001_1_pmil length: 340\n",
      "Video hundvalp_001_1_pmil length: 1505\n",
      "Video left_hand_on_top_of_the_battery_neutral_location_001_1_pmil length: 479\n",
      "Video parti-vill-kalla-grupp-for-terrorister-5-bosattarna_001_1_pmil length: 540\n",
      "Video touch_chin_with_index_finger_side_of_the_chin_far_from_active_hand_active_hand_001_1_pmil length: 942\n",
      "Video ogon_cheeks_active_hand_in_each_place_001_1_pmil length: 732\n",
      "456\n",
      "19\n",
      "19\n",
      "(456, 51)\n",
      "(387, 51)\n",
      "(437, 51)\n",
      "(338, 51)\n",
      "(1067, 51)\n",
      "(351, 51)\n",
      "(404, 51)\n",
      "(1876, 51)\n",
      "(1194, 51)\n",
      "(231, 51)\n",
      "(2112, 51)\n",
      "(480, 51)\n",
      "(344, 51)\n",
      "(340, 51)\n",
      "(1505, 51)\n",
      "(479, 51)\n",
      "(540, 51)\n",
      "(942, 51)\n",
      "(732, 51)\n",
      "\n",
      "Percentage of REAL occlusions: 0.03904326415758002\n",
      "\n",
      "Video 0 length: 456\n",
      "Video 1 length: 387\n",
      "Video 2 length: 437\n",
      "Video 3 length: 338\n",
      "Video 4 length: 1067\n",
      "Video 5 length: 351\n",
      "Video 6 length: 404\n",
      "Video 7 length: 1876\n",
      "Video 8 length: 1194\n",
      "Video 9 length: 231\n",
      "Video 10 length: 2112\n",
      "Video 11 length: 480\n",
      "Video 12 length: 344\n",
      "Video 13 length: 340\n",
      "Video 14 length: 1505\n",
      "Video 15 length: 479\n",
      "Video 16 length: 540\n",
      "Video 17 length: 942\n",
      "Video 18 length: 732\n",
      "\n",
      "Percentage of occlusions: 0.21660218079493493\n",
      "\n",
      "\n",
      "Percentage of masked data: 0.1775589166373549\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_loader = create_dataloader(train_data, batch_size=8, shuffle=True)\n",
    "val_loader = create_dataloader(val_data, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=3, dropout=0.3, bidirectional=True):\n",
    "        super(BiLSTMModel, self).__init__()\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size,\n",
    "            hidden_size,\n",
    "            num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=bidirectional,\n",
    "            dropout=dropout if num_layers > 1 else 0)\n",
    "        self.fc = nn.Linear(hidden_size * self.num_directions, output_size)\n",
    "        self.fc2 = nn.Linear(output_size, input_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "    def forward(self, x, seq_lens):\n",
    "        # print(\"forward x.shape: \", x.shape, \"seq_lens_sort.shape: \", seq_lens.shape)  # forward x.shape:  torch.Size([8, 2060, 51]) seq_lens_sort.shape:  torch.Size([8])\n",
    "    \n",
    "        # Check if single sequence\n",
    "        if x.size(0) == 1:\n",
    "            # Directly pass the single sequence through the LSTM\n",
    "            y, _ = self.lstm(x)  # No packing needed\n",
    "            y = self.fc(y)\n",
    "            y = self.act(y)\n",
    "            y = self.dropout(y)\n",
    "            y = self.fc2(y)\n",
    "            return y\n",
    "        \n",
    "        # sort input by descending length\n",
    "        _, idx_sort = torch.sort(seq_lens, dim=0, descending=True)\n",
    "        idx_sort = idx_sort.to(device)\n",
    "        _, idx_unsort = torch.sort(idx_sort, dim=0)\n",
    "        idx_unsort = idx_unsort.to(device)\n",
    "        x_sort = torch.index_select(x, dim=0, index=idx_sort)\n",
    "        x_sort = x_sort.to(device)\n",
    "        seq_lens_sort = torch.index_select(seq_lens, dim=0, index=idx_sort)\n",
    "        seq_lens_sort = seq_lens_sort.to(device)\n",
    "\n",
    "        seq_lens_sort = seq_lens_sort.cpu()\n",
    "        x_packed = pack_padded_sequence(\n",
    "            x_sort, seq_lens_sort, batch_first=True)  #, enforce_sorted=False)\n",
    "        \n",
    "        # forward x_packed.data.shape:  torch.Size([6653, 51])\n",
    "        # print(\"forward x_packed.data.shape: \", x_packed.data.shape)\n",
    "\n",
    "        # pass through rnn\n",
    "        y_packed, _ = self.lstm(x_packed)\n",
    "\n",
    "        # unpack output\n",
    "        y_sort, length = pad_packed_sequence(y_packed.to(device), batch_first=True)\n",
    "        # unsort output to original order\n",
    "        y = torch.index_select(y_sort, dim=0, index=idx_unsort)\n",
    "\n",
    "        out = self.fc(y)\n",
    "        out = self.act(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc2(out)\n",
    "\n",
    "        #print(\"forward out.shape: \", out.shape)\n",
    "        # forward out.shape:  torch.Size([8, 51])\n",
    "        return out\n",
    "\n",
    "\n",
    "def masked_loss(output, target, labels, mask, loss_fn, epoch):\n",
    "    # refuralize between frame\n",
    "    loss = loss_fn(output, target)\n",
    "    # Warm-up for the first 4 epochs on ground truth data\n",
    "    #if epoch < 4:  \n",
    "        # loss over only fake occlusions\n",
    "        # mask = 1 for real, 0 for fake data\n",
    "        # only fake occlusions = all zeros in mask\n",
    "        # flip the mask to get only fake occlusions\n",
    "    #    mask = mask.unsqueeze(-1)\n",
    "    #    return (loss * (1 - mask)).sum() / (1 - mask).sum()\n",
    "    # else:\n",
    "        # loss over all occlusions\n",
    "        # labels = 1 for occluded, 0 for not occluded\n",
    "    labels = labels.unsqueeze(-1)\n",
    "    return (loss * labels).sum() / labels.sum()\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs, writer):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "\n",
    "        running_loss = 0.0\n",
    "        # data, labels, mask, sequence_lengths\n",
    "        for batch_features, batch_labels, batch_mask, batch_lengths in train_loader:\n",
    "            batch_features = batch_features.to(device)\n",
    "            batch_labels = batch_labels.to(device)\n",
    "            batch_mask = batch_mask.to(device)\n",
    "            batch_lengths = batch_lengths.to(device)\n",
    "\n",
    "            # Forward pass \n",
    "            #print(\"batch_features.shape: \", batch_features.shape, \"batch_mask.shape: \", batch_mask.shape)\n",
    "            #print(\"batch_mask.unsqueeze(-1).shape: \", batch_mask.unsqueeze(-1).shape)\n",
    "            batch_features_masked = batch_features * batch_mask.unsqueeze(-1)\n",
    "            batch_features_masked = batch_features_masked.to(device)\n",
    "            outputs = model(batch_features_masked, batch_lengths)\n",
    "            #print(\"outputs.shape: \", outputs.shape)\n",
    "            \n",
    "            # Compute masked loss\n",
    "            #print(\"batch_labels.shape: \", batch_labels.shape)\n",
    "            loss = masked_loss(outputs, batch_features, batch_labels, batch_mask, criterion, epoch)\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        running_loss /= len(train_loader)\n",
    "        print(f\"Epoch {epoch + 1}, Average loss: {running_loss:.4f}\")\n",
    "        writer.add_scalar(\"Loss/train\", running_loss, epoch)\n",
    "\n",
    "        # Validation loss\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = 0\n",
    "            for batch_features, batch_labels, batch_mask, batch_lengths in val_loader:\n",
    "                batch_features = batch_features.to(device)\n",
    "                batch_labels = batch_labels.to(device)\n",
    "                batch_mask = batch_mask.to(device)\n",
    "                batch_lengths = batch_lengths.to(device)\n",
    "            \n",
    "                batch_features_masked = batch_features * batch_mask.unsqueeze(-1)\n",
    "                batch_features_masked = batch_features_masked.to(device)\n",
    "                # Forward pass on validation data\n",
    "                outputs = model(batch_features_masked, batch_lengths)\n",
    "\n",
    "                # Compute masked loss\n",
    "                loss = masked_loss(outputs, batch_features, batch_labels, batch_mask, criterion, epoch)\n",
    "                val_loss += loss.item()\n",
    "            \n",
    "            val_loss /= len(val_loader)\n",
    "\n",
    "            print(f\"Average validation Loss: {val_loss:.4f}\")\n",
    "            writer.add_scalar(\"Loss/val\", val_loss, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorboard\n",
      "  Using cached tensorboard-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting absl-py>=0.4 (from tensorboard)\n",
      "  Using cached absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting grpcio>=1.48.2 (from tensorboard)\n",
      "  Downloading grpcio-1.68.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.9 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard)\n",
      "  Using cached Markdown-3.7-py3-none-any.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: numpy>=1.12.0 in ./.conda/lib/python3.11/site-packages (from tensorboard) (1.26.4)\n",
      "Requirement already satisfied: packaging in ./.conda/lib/python3.11/site-packages (from tensorboard) (24.2)\n",
      "Collecting protobuf!=4.24.0,>=3.19.6 (from tensorboard)\n",
      "  Downloading protobuf-5.29.1-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in ./.conda/lib/python3.11/site-packages (from tensorboard) (75.1.0)\n",
      "Requirement already satisfied: six>1.9 in ./.conda/lib/python3.11/site-packages (from tensorboard) (1.16.0)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard)\n",
      "  Using cached tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard)\n",
      "  Downloading werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in ./.conda/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.3)\n",
      "Using cached tensorboard-2.18.0-py3-none-any.whl (5.5 MB)\n",
      "Using cached absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "Downloading grpcio-1.68.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached Markdown-3.7-py3-none-any.whl (106 kB)\n",
      "Downloading protobuf-5.29.1-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n",
      "Using cached tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "Downloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "Installing collected packages: werkzeug, tensorboard-data-server, protobuf, markdown, grpcio, absl-py, tensorboard\n",
      "Successfully installed absl-py-2.1.0 grpcio-1.68.1 markdown-3.7 protobuf-5.29.1 tensorboard-2.18.0 tensorboard-data-server-0.7.2 werkzeug-3.1.3\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Average loss: 21.7245\n",
      "Average validation Loss: 2.4141\n",
      "Epoch 2, Average loss: 23.2867\n",
      "Average validation Loss: 2.0839\n",
      "Epoch 3, Average loss: 14.5587\n",
      "Average validation Loss: 1.9374\n",
      "Epoch 4, Average loss: 18.3226\n",
      "Average validation Loss: 1.8847\n",
      "Epoch 5, Average loss: 14.6346\n",
      "Average validation Loss: 1.8451\n",
      "Epoch 6, Average loss: 13.4768\n",
      "Average validation Loss: 1.8387\n",
      "Epoch 7, Average loss: 11.6035\n",
      "Average validation Loss: 1.8191\n",
      "Epoch 8, Average loss: 16.5129\n",
      "Average validation Loss: 1.8046\n",
      "Epoch 9, Average loss: 7.6271\n",
      "Average validation Loss: 1.7767\n",
      "Epoch 10, Average loss: 7.3603\n",
      "Average validation Loss: 1.7484\n",
      "Epoch 11, Average loss: 5.2440\n",
      "Average validation Loss: 1.7486\n",
      "Epoch 12, Average loss: 5.0275\n",
      "Average validation Loss: 1.7254\n",
      "Epoch 13, Average loss: 4.5403\n",
      "Average validation Loss: 1.7068\n",
      "Epoch 14, Average loss: 6.4754\n",
      "Average validation Loss: 1.6961\n",
      "Epoch 15, Average loss: 5.3203\n",
      "Average validation Loss: 1.6539\n",
      "Epoch 16, Average loss: 4.2665\n",
      "Average validation Loss: 1.6367\n",
      "Epoch 17, Average loss: 4.2402\n",
      "Average validation Loss: 1.5929\n",
      "Epoch 18, Average loss: 5.0804\n",
      "Average validation Loss: 1.6263\n",
      "Epoch 19, Average loss: 4.5565\n",
      "Average validation Loss: 1.5895\n",
      "Epoch 20, Average loss: 4.1537\n",
      "Average validation Loss: 1.5646\n",
      "Epoch 21, Average loss: 4.6049\n",
      "Average validation Loss: 1.5620\n",
      "Epoch 22, Average loss: 3.5266\n",
      "Average validation Loss: 1.5270\n",
      "Epoch 23, Average loss: 3.6464\n",
      "Average validation Loss: 1.5118\n",
      "Epoch 24, Average loss: 4.4401\n",
      "Average validation Loss: 1.5231\n",
      "Epoch 25, Average loss: 8.1764\n",
      "Average validation Loss: 1.5092\n",
      "Epoch 26, Average loss: 3.8411\n",
      "Average validation Loss: 1.6228\n",
      "Epoch 27, Average loss: 11.1288\n",
      "Average validation Loss: 1.5052\n",
      "Epoch 28, Average loss: 4.7275\n",
      "Average validation Loss: 1.5182\n",
      "Epoch 29, Average loss: 7.1516\n",
      "Average validation Loss: 1.5191\n",
      "Epoch 30, Average loss: 4.1916\n",
      "Average validation Loss: 1.4951\n",
      "Epoch 31, Average loss: 3.1371\n",
      "Average validation Loss: 1.5215\n",
      "Epoch 32, Average loss: 2.5059\n",
      "Average validation Loss: 1.5156\n",
      "Epoch 33, Average loss: 2.7415\n",
      "Average validation Loss: 1.4720\n",
      "Epoch 34, Average loss: 2.4789\n",
      "Average validation Loss: 1.4354\n",
      "Epoch 35, Average loss: 2.5769\n",
      "Average validation Loss: 1.4386\n",
      "Epoch 36, Average loss: 2.5133\n",
      "Average validation Loss: 1.4309\n",
      "Epoch 37, Average loss: 2.5026\n",
      "Average validation Loss: 1.4174\n",
      "Epoch 38, Average loss: 2.6525\n",
      "Average validation Loss: 1.4139\n",
      "Epoch 39, Average loss: 2.6245\n",
      "Average validation Loss: 1.4106\n",
      "Epoch 40, Average loss: 2.0811\n",
      "Average validation Loss: 1.4098\n",
      "Epoch 41, Average loss: 2.3626\n",
      "Average validation Loss: 1.3983\n",
      "Epoch 42, Average loss: 2.1246\n",
      "Average validation Loss: 1.3891\n",
      "Epoch 43, Average loss: 2.3191\n",
      "Average validation Loss: 1.3837\n",
      "Epoch 44, Average loss: 2.1746\n",
      "Average validation Loss: 1.3780\n",
      "Epoch 45, Average loss: 2.1449\n",
      "Average validation Loss: 1.3832\n",
      "Epoch 46, Average loss: 2.0397\n",
      "Average validation Loss: 1.3926\n",
      "Epoch 47, Average loss: 1.8100\n",
      "Average validation Loss: 1.4133\n",
      "Epoch 48, Average loss: 1.9140\n",
      "Average validation Loss: 1.3824\n",
      "Epoch 49, Average loss: 2.2216\n",
      "Average validation Loss: 1.3767\n",
      "Epoch 50, Average loss: 2.1133\n",
      "Average validation Loss: 1.4105\n",
      "Epoch 51, Average loss: 2.0409\n",
      "Average validation Loss: 1.4032\n",
      "Epoch 52, Average loss: 2.0555\n",
      "Average validation Loss: 1.3706\n",
      "Epoch 53, Average loss: 1.5801\n",
      "Average validation Loss: 1.3778\n",
      "Epoch 54, Average loss: 1.7198\n",
      "Average validation Loss: 1.3870\n",
      "Epoch 55, Average loss: 1.6761\n",
      "Average validation Loss: 1.3664\n",
      "Epoch 56, Average loss: 1.4789\n",
      "Average validation Loss: 1.3732\n",
      "Epoch 57, Average loss: 1.7271\n",
      "Average validation Loss: 1.3543\n",
      "Epoch 58, Average loss: 1.4626\n",
      "Average validation Loss: 1.3796\n",
      "Epoch 59, Average loss: 1.4626\n",
      "Average validation Loss: 1.3536\n",
      "Epoch 60, Average loss: 1.9955\n",
      "Average validation Loss: 1.3619\n",
      "Epoch 61, Average loss: 1.4120\n",
      "Average validation Loss: 1.3515\n",
      "Epoch 62, Average loss: 1.8178\n",
      "Average validation Loss: 1.3916\n",
      "Epoch 63, Average loss: 1.3699\n",
      "Average validation Loss: 1.3632\n",
      "Epoch 64, Average loss: 1.5571\n",
      "Average validation Loss: 1.3534\n",
      "Epoch 65, Average loss: 1.5547\n",
      "Average validation Loss: 1.3357\n",
      "Epoch 66, Average loss: 1.7682\n",
      "Average validation Loss: 1.3684\n",
      "Epoch 67, Average loss: 1.4662\n",
      "Average validation Loss: 1.3591\n",
      "Epoch 68, Average loss: 1.4949\n",
      "Average validation Loss: 1.7370\n",
      "Epoch 69, Average loss: 1.8081\n",
      "Average validation Loss: 1.3969\n",
      "Epoch 70, Average loss: 1.9333\n",
      "Average validation Loss: 1.4103\n",
      "Epoch 71, Average loss: 1.7536\n",
      "Average validation Loss: 1.3487\n",
      "Epoch 72, Average loss: 1.4508\n",
      "Average validation Loss: 1.3537\n",
      "Epoch 73, Average loss: 1.4491\n",
      "Average validation Loss: 1.3542\n",
      "Epoch 74, Average loss: 1.4607\n",
      "Average validation Loss: 1.3716\n",
      "Epoch 75, Average loss: 1.3875\n",
      "Average validation Loss: 1.3675\n",
      "Epoch 76, Average loss: 1.5181\n",
      "Average validation Loss: 1.3546\n",
      "Epoch 77, Average loss: 1.5774\n",
      "Average validation Loss: 1.3415\n",
      "Epoch 78, Average loss: 1.4782\n",
      "Average validation Loss: 1.3469\n",
      "Epoch 79, Average loss: 1.2144\n",
      "Average validation Loss: 1.3394\n",
      "Epoch 80, Average loss: 1.2527\n",
      "Average validation Loss: 1.3640\n",
      "Epoch 81, Average loss: 1.3251\n",
      "Average validation Loss: 1.3405\n",
      "Epoch 82, Average loss: 1.3128\n",
      "Average validation Loss: 1.3498\n",
      "Epoch 83, Average loss: 1.3940\n",
      "Average validation Loss: 1.3377\n",
      "Epoch 84, Average loss: 1.4069\n",
      "Average validation Loss: 1.4272\n",
      "Epoch 85, Average loss: 2.0511\n",
      "Average validation Loss: 1.3840\n",
      "Epoch 86, Average loss: 1.4909\n",
      "Average validation Loss: 1.3536\n",
      "Epoch 87, Average loss: 1.5629\n",
      "Average validation Loss: 1.3495\n",
      "Epoch 88, Average loss: 1.5188\n",
      "Average validation Loss: 1.3755\n",
      "Epoch 89, Average loss: 1.3359\n",
      "Average validation Loss: 1.3264\n",
      "Epoch 90, Average loss: 1.3924\n",
      "Average validation Loss: 1.3296\n",
      "Epoch 91, Average loss: 1.3049\n",
      "Average validation Loss: 1.3671\n",
      "Epoch 92, Average loss: 1.4055\n",
      "Average validation Loss: 1.3511\n",
      "Epoch 93, Average loss: 1.4843\n",
      "Average validation Loss: 1.3271\n",
      "Epoch 94, Average loss: 1.5003\n",
      "Average validation Loss: 1.3548\n",
      "Epoch 95, Average loss: 1.3200\n",
      "Average validation Loss: 1.3667\n",
      "Epoch 96, Average loss: 1.2835\n",
      "Average validation Loss: 1.3566\n",
      "Epoch 97, Average loss: 1.5233\n",
      "Average validation Loss: 1.3153\n",
      "Epoch 98, Average loss: 1.5543\n",
      "Average validation Loss: 1.3312\n",
      "Epoch 99, Average loss: 1.3779\n",
      "Average validation Loss: 1.3248\n",
      "Epoch 100, Average loss: 1.9943\n",
      "Average validation Loss: 1.3138\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()\n",
    "\n",
    "\n",
    "# Example setup\n",
    "if __name__ == \"__main__\":\n",
    "    # Example data\n",
    "    #num_samples = 100\n",
    "    #time_steps = 50\n",
    "    #features = 10\n",
    "    #data = np.random.rand(num_samples, time_steps, features)\n",
    "    #labels = np.random.rand(num_samples, 1)\n",
    "\n",
    "    # Preprocess data\n",
    "    # standardized_data, scaler = fit_and_standardize(data)\n",
    "\n",
    "    # Create DataLoader\n",
    "    batch_size = 8\n",
    "\n",
    "    # Initialize model, loss, and optimizer\n",
    "    input_size = 51\n",
    "    hidden_size = 128  # 128 on 80 epochs looked alright\n",
    "    num_layers = 2  # 3 initially\n",
    "    dropout = 0.4\n",
    "    output_size = 51\n",
    "\n",
    "    model = BiLSTMModel(input_size, hidden_size, output_size, num_layers=num_layers, dropout=dropout).to(device)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.MSELoss(reduction='none')\n",
    "\n",
    "    # Train model\n",
    "    epochs = 100\n",
    "    train_model(model, train_loader, val_loader, criterion, optimizer, epochs, writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average test Loss: 0.5661\n"
     ]
    }
   ],
   "source": [
    "def test_model(model, test_loader, criterion, writer):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_loss = 0\n",
    "        for batch_features, batch_labels, batch_mask, batch_lengths in test_loader:\n",
    "            batch_features = batch_features.to(device)\n",
    "            batch_labels = batch_labels.to(device)\n",
    "            batch_mask = batch_mask.to(device)\n",
    "            batch_lengths = batch_lengths.to(device)\n",
    "            \n",
    "            batch_features_masked = batch_features * batch_mask.unsqueeze(-1)\n",
    "            batch_features_masked = batch_features_masked.to(device)\n",
    "            # Forward pass on test data\n",
    "            outputs = model(batch_features_masked, batch_lengths)\n",
    "\n",
    "            # Compute masked loss\n",
    "            loss = masked_loss(outputs, batch_features, batch_labels, batch_mask, criterion, epoch=None)\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "        test_loss /= len(test_loader)\n",
    "\n",
    "        print(f\"Average test Loss: {test_loss:.4f}\")\n",
    "        writer.add_scalar(\"Loss/test\", test_loss)\n",
    "\n",
    "test_model(model, test_loader, criterion, writer)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "input_size = 51\n",
    "hidden_size = 128 \n",
    "num_layers = 3\n",
    "dropout = 0.4\n",
    "output_size = 51\n",
    "epochs = 80\n",
    "```\n",
    "\n",
    "Average test Loss: 0.6328 in `bilstm-40dropout-80.pt`\n",
    "\n",
    "```\n",
    "input_size = 51\n",
    "hidden_size = 128  # 128 on 80 epochs looked alright\n",
    "num_layers = 2  # 3 initially\n",
    "dropout = 0.4\n",
    "output_size = 51\n",
    "epochs = 100\n",
    "```\n",
    "\n",
    "Average test Loss: 0.5661 in `bilstm-40dropout-2layers-100.pt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow installation not found - running with reduced feature set.\n",
      "\n",
      "NOTE: Using experimental fast data loading logic. To disable, pass\n",
      "    \"--load_fast=false\" and report issues on GitHub. More details:\n",
      "    https://github.com/tensorflow/tensorboard/issues/4784\n",
      "\n",
      "TensorBoard 2.18.0 at http://signbot1:6006/ (Press CTRL+C to quit)\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "# launch tensorboard\n",
    "!tensorboard --logdir=runs --bind_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "torch.save(model.state_dict(), f\"bilstm-40dropout-{num_layers}layers-{epochs}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2237928/4006193575.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load saved model from state dict\n",
    "input_size = 51\n",
    "hidden_size = 128  # 128 on 80 epochs looked alright\n",
    "num_layers = 2  # 3 initially\n",
    "dropout = 0.4\n",
    "output_size = 51\n",
    "epochs = 100\n",
    "\n",
    "model_path = \"bilstm-40dropout-2layers-100.pt\"\n",
    "model = BiLSTMModel(input_size, hidden_size, output_size, num_layers=num_layers, dropout=dropout).to(device)\n",
    "model.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BrowDownLeft', 'BrowDownRight', 'BrowInnerUp', 'BrowOuterUpLeft', 'BrowOuterUpRight', 'CheekPuff', 'CheekSquintLeft', 'CheekSquintRight', 'EyeBlinkLeft', 'EyeBlinkRight', 'EyeLookDownLeft', 'EyeLookDownRight', 'EyeLookInLeft', 'EyeLookInRight', 'EyeLookOutLeft', 'EyeLookOutRight', 'EyeLookUpLeft', 'EyeLookUpRight', 'EyeSquintLeft', 'EyeSquintRight', 'EyeWideLeft', 'EyeWideRight', 'JawForward', 'JawLeft', 'JawOpen', 'JawRight', 'MouthClose', 'MouthDimpleLeft', 'MouthDimpleRight', 'MouthFrownLeft', 'MouthFrownRight', 'MouthFunnel', 'MouthLeft', 'MouthLowerDownLeft', 'MouthLowerDownRight', 'MouthPressLeft', 'MouthPressRight', 'MouthPucker', 'MouthRight', 'MouthRollLower', 'MouthRollUpper', 'MouthShrugLower', 'MouthShrugUpper', 'MouthSmileLeft', 'MouthSmileRight', 'MouthStretchLeft', 'MouthStretchRight', 'MouthUpperUpLeft', 'MouthUpperUpRight', 'NoseSneerLeft', 'NoseSneerRight']\n",
      "125\n",
      "0\n",
      "dict_keys(['BrowDownLeft', 'BrowDownRight', 'BrowInnerUp', 'BrowOuterUpLeft', 'BrowOuterUpRight', 'CheekPuff', 'CheekSquintLeft', 'CheekSquintRight', 'EyeBlinkLeft', 'EyeBlinkRight', 'EyeLookDownLeft', 'EyeLookDownRight', 'EyeLookInLeft', 'EyeLookInRight', 'EyeLookOutLeft', 'EyeLookOutRight', 'EyeLookUpLeft', 'EyeLookUpRight', 'EyeSquintLeft', 'EyeSquintRight', 'EyeWideLeft', 'EyeWideRight', 'JawForward', 'JawLeft', 'JawOpen', 'JawRight', 'MouthClose', 'MouthDimpleLeft', 'MouthDimpleRight', 'MouthFrownLeft', 'MouthFrownRight', 'MouthFunnel', 'MouthLeft', 'MouthLowerDownLeft', 'MouthLowerDownRight', 'MouthPressLeft', 'MouthPressRight', 'MouthPucker', 'MouthRight', 'MouthRollLower', 'MouthRollUpper', 'MouthShrugLower', 'MouthShrugUpper', 'MouthSmileLeft', 'MouthSmileRight', 'MouthStretchLeft', 'MouthStretchRight', 'MouthUpperUpLeft', 'MouthUpperUpRight', 'NoseSneerLeft', 'NoseSneerRight', 'Timecode', 'BlendshapeCount', 'velocity', 'smoothed_velocity', 'occluded'])\n",
      "Predicting occluded blendshapes for video: static_ROOF_fingers_contact_at_45_degrees_neutral_location_001_1_pmil\n",
      "Predicting occluded blendshapes for video: varg_001_1_pmil\n",
      "Predicting occluded blendshapes for video: bi_005_5_pmil\n",
      "Predicting occluded blendshapes for video: fran_001_1_pmil\n",
      "Predicting occluded blendshapes for video: parti-vill-kalla-grupp-for-terrorister-1-det_001_1_pmil\n",
      "Predicting occluded blendshapes for video: parti-vill-kalla-grupp-for-terrorister-4-nu_002_2_pmil\n",
      "Predicting occluded blendshapes for video: katt_001_1_pmil\n",
      "Predicting occluded blendshapes for video: bok_002_2_pmil\n",
      "Predicting occluded blendshapes for video: varg_003_3_pmil\n",
      "Predicting occluded blendshapes for video: parti-vill-kalla-grupp-for-terrorister-1-det_003_3_pmil\n",
      "Predicting occluded blendshapes for video: djur_001_1_pmil\n",
      "Predicting occluded blendshapes for video: abborre_002_2_pmil\n",
      "Predicting occluded blendshapes for video: parti-vill-kalla-grupp-for-terrorister-5-bosattarna_003_3_pmil\n",
      "Predicting occluded blendshapes for video: parti-vill-kalla-grupp-for-terrorister-7-det_001_1_pmil\n",
      "Predicting occluded blendshapes for video: hundvalp_001_1_pmil\n",
      "Predicting occluded blendshapes for video: left_hand_on_top_of_the_battery_neutral_location_001_1_pmil\n",
      "Predicting occluded blendshapes for video: parti-vill-kalla-grupp-for-terrorister-5-bosattarna_001_1_pmil\n",
      "Predicting occluded blendshapes for video: touch_chin_with_index_finger_side_of_the_chin_far_from_active_hand_active_hand_001_1_pmil\n",
      "Predicting occluded blendshapes for video: ogon_cheeks_active_hand_in_each_place_001_1_pmil\n",
      "Predicting occluded blendshapes for video: mussla_002_2_pmil\n",
      "Predicting occluded blendshapes for video: right_hand_above_belly_button_001_1_pmil\n",
      "Predicting occluded blendshapes for video: static_ROOF_fingers_contact_at_45_degrees_left_top_location_001_1_pmil\n",
      "Predicting occluded blendshapes for video: skara_001_1_pmil\n",
      "Predicting occluded blendshapes for video: minister-flydde-fran-tomater-story_001_1_pmil\n",
      "Predicting occluded blendshapes for video: hundvalp_003_3_pmil\n",
      "Predicting occluded blendshapes for video: static_closed_book_left_bottom_location_001_1_pmil\n",
      "Predicting occluded blendshapes for video: stada_001_1_pmil\n",
      "Predicting occluded blendshapes for video: tro_001_1_pmil\n",
      "Predicting occluded blendshapes for video: ensam_002_2_pmil\n",
      "Predicting occluded blendshapes for video: baver_002_2_pmil\n",
      "Predicting occluded blendshapes for video: parti-vill-kalla-grupp-for-terrorister-2-men_002_2_pmil\n",
      "Predicting occluded blendshapes for video: static_closed_book_right_top_location_001_1_pmil\n",
      "Predicting occluded blendshapes for video: till_001_1_pmil\n",
      "Predicting occluded blendshapes for video: sko_001_1_pmil\n",
      "Predicting occluded blendshapes for video: right_hand_on_top_of_the_battery_neutral_location_001_1_pmil\n",
      "Predicting occluded blendshapes for video: halsband_002_2_pmil\n",
      "Predicting occluded blendshapes for video: groda_002_2_pmil\n",
      "Predicting occluded blendshapes for video: kalkon_002_2_pmil\n",
      "Predicting occluded blendshapes for video: static_ROOF_fingers_contact_at_45_degrees_right_top_location_001_1_pmil\n",
      "Predicting occluded blendshapes for video: tip_of_the_nose_touch_and_hold_active_hand_001_1_pmil\n",
      "Predicting occluded blendshapes for video: min_001_1_pmil\n",
      "Predicting occluded blendshapes for video: touch_chin_with_index_finger_middle_of_the_chin_active_hand_001_1_pmil\n",
      "Predicting occluded blendshapes for video: nytt-forslag-om-invandrare-1-regeringen_002_2_pmil\n",
      "Predicting occluded blendshapes for video: bi_002_2_pmil\n",
      "Predicting occluded blendshapes for video: nytt-forslag-om-invandrare-3-nu_001_1_pmil\n",
      "Predicting occluded blendshapes for video: forehead-to-chin_NOT_UNDERSTAND_R_chin_forehead_active_hand_in_each_place_001_1_pmil\n",
      "Predicting occluded blendshapes for video: glad_002_2_pmil\n",
      "Predicting occluded blendshapes for video: kobra_002_2_pmil\n",
      "Predicting occluded blendshapes for video: static_JOBBA_left_bottom_location_001_1_pmil\n",
      "Predicting occluded blendshapes for video: parti-vill-kalla-grupp-for-terrorister-6-partiet_001_1_pmil\n",
      "Predicting occluded blendshapes for video: fjaril_001_1_pmil\n",
      "Predicting occluded blendshapes for video: hackspett_002_2_pmil\n",
      "Predicting occluded blendshapes for video: parti-vill-kalla-grupp-for-terrorister-3-en_001_1_pmil\n",
      "Predicting occluded blendshapes for video: nytt-forslag-om-invandrare-4-politikerna_001_1_pmil\n",
      "Predicting occluded blendshapes for video: matt_001_1_pmil\n",
      "Predicting occluded blendshapes for video: static_closed_book_left_top_location_001_1_pmil\n",
      "Predicting occluded blendshapes for video: fasting_001_1_pmil\n",
      "Predicting occluded blendshapes for video: static_JOBBA_neutral_location_001_1_pmil\n",
      "Predicting occluded blendshapes for video: face-calib_002_2_pmil\n",
      "Predicting occluded blendshapes for video: static_ROOF_fingers_contact_at_45_degrees_in_front_of_the_face_location_001_1_pmil\n",
      "Inhomogeneous number of blendshapes in static_ROOF_fingers_contact_at_45_degrees_in_front_of_the_face_location_001_1_pmil\n",
      "Predicting occluded blendshapes for video: static_JOBBA_left_top_location_001_1_pmil\n",
      "Predicting occluded blendshapes for video: parti-vill-kalla-grupp-for-terrorister-5-bosattarna_004_4_pmil\n",
      "Predicting occluded blendshapes for video: t-pose_001_1_pmil\n",
      "Predicting occluded blendshapes for video: ensam_001_1_pmil\n",
      "Predicting occluded blendshapes for video: right_hand_resting_on_the_chest_chest_001_1_pmil\n",
      "Predicting occluded blendshapes for video: fagel_001_1_pmil\n",
      "Predicting occluded blendshapes for video: hand_alphabet_001_1_pmil\n",
      "Predicting occluded blendshapes for video: varg_002_2_pmil\n",
      "Predicting occluded blendshapes for video: parti-vill-kalla-grupp-for-terrorister-2-men_001_1_pmil\n",
      "Predicting occluded blendshapes for video: baver_001_1_pmil\n",
      "Predicting occluded blendshapes for video: groda_003_3_pmil\n",
      "Predicting occluded blendshapes for video: static_JOBBA_right_bottom_location_001_1_pmil\n",
      "Predicting occluded blendshapes for video: left_hand_resting_on_the_chest_chest_001_1_pmil\n",
      "Predicting occluded blendshapes for video: bi_003_3_pmil\n",
      "Predicting occluded blendshapes for video: apa_001_1_pmil\n",
      "Predicting occluded blendshapes for video: anka_001_1_pmil\n",
      "Predicting occluded blendshapes for video: static_JOBBA_right_top_location_001_1_pmil\n",
      "Predicting occluded blendshapes for video: parti-vill-kalla-grupp-for-terrorister-2-men_003_3_pmil\n",
      "Predicting occluded blendshapes for video: baver_003_3_pmil\n",
      "Predicting occluded blendshapes for video: halsband_001_1_pmil\n",
      "Predicting occluded blendshapes for video: groda_001_1_pmil\n",
      "Predicting occluded blendshapes for video: kalkon_001_1_pmil\n",
      "Predicting occluded blendshapes for video: bi_001_1_pmil\n",
      "Predicting occluded blendshapes for video: nytt-forslag-om-invandrare-1-regeringen_001_1_pmil\n",
      "Predicting occluded blendshapes for video: static_ROOF_fingers_contact_at_45_degrees_right_bottom_location_001_1_pmil\n",
      "Predicting occluded blendshapes for video: bock_001_1_pmil\n",
      "Predicting occluded blendshapes for video: kobra_001_1_pmil\n",
      "Predicting occluded blendshapes for video: static_JOBBA_in_front_of_the_face_location_001_1_pmil\n",
      "Predicting occluded blendshapes for video: antilop_001_1_pmil\n",
      "Predicting occluded blendshapes for video: glad_001_1_pmil\n",
      "Predicting occluded blendshapes for video: hackspett_001_1_pmil\n",
      "Predicting occluded blendshapes for video: static_closed_book_left_top_location_002_2_pmil\n",
      "Predicting occluded blendshapes for video: touch_side_of_the_forehead_on_active_hand_side_active_hand_001_1_pmil\n",
      "Predicting occluded blendshapes for video: nytt-forslag-om-invandrare-2-de_001_1_pmil\n",
      "Predicting occluded blendshapes for video: tack_001_1_pmil\n",
      "Predicting occluded blendshapes for video: krama_001_1_pmil\n",
      "Predicting occluded blendshapes for video: face-calib_001_1_pmil\n",
      "Predicting occluded blendshapes for video: a-pose_001_1_pmil\n",
      "Predicting occluded blendshapes for video: parti-vill-kalla-grupp-for-terrorister-2-men_004_4_pmil\n",
      "Predicting occluded blendshapes for video: kontor_001_1_pmil\n",
      "Predicting occluded blendshapes for video: nytt-forslag-om-invandrare-5-en_001_1_pmil\n",
      "Predicting occluded blendshapes for video: parti-vill-kalla-grupp-for-terrorister-1-det_002_2_pmil\n",
      "Predicting occluded blendshapes for video: parti-vill-kalla-grupp-for-terrorister-4-nu_001_1_pmil\n",
      "Predicting occluded blendshapes for video: static_closed_book_right_bottom_location_001_1_pmil\n",
      "Predicting occluded blendshapes for video: bok_001_1_pmil\n",
      "Predicting occluded blendshapes for video: static_ROOF_fingers_contact_at_45_degrees_left_bottom_location_001_1_pmil\n",
      "Predicting occluded blendshapes for video: bi_004_4_pmil\n",
      "Predicting occluded blendshapes for video: parti-vill-kalla-grupp-for-terrorister-4-nu_003_3_pmil\n",
      "Predicting occluded blendshapes for video: abborre_001_1_pmil\n",
      "Predicting occluded blendshapes for video: touch_chin_with_index_finger_side_of_the_chin_close_to_active_hand_active_hand_001_1_pmil\n",
      "Predicting occluded blendshapes for video: en-1_001_1_pmil\n",
      "Predicting occluded blendshapes for video: skola_001_1_pmil\n",
      "Predicting occluded blendshapes for video: chinchilla_001_1_pmil\n",
      "Predicting occluded blendshapes for video: static_closed_book_neutral_location_001_1_pmil\n",
      "Predicting occluded blendshapes for video: touch_side_of_the_cheek_with_a_palm_on_active_hand_side_active_hand_001_1_pmil\n",
      "Predicting occluded blendshapes for video: hundvalp_002_2_pmil\n",
      "Predicting occluded blendshapes for video: parti-vill-kalla-grupp-for-terrorister-5-bosattarna_002_2_pmil\n",
      "Predicting occluded blendshapes for video: left_hand_above_belly_button_001_1_pmil\n",
      "Predicting occluded blendshapes for video: static_closed_book_in_front_of_the_face_location_001_1_pmil\n",
      "Predicting occluded blendshapes for video: giftorm_001_1_pmil\n",
      "Predicting occluded blendshapes for video: right_hand_above_belly_button_002_2_pmil\n",
      "Predicting occluded blendshapes for video: mussla_001_1_pmil\n",
      "Predicting occluded blendshapes for video: dalig_001_1_pmil\n",
      "Predicting occluded blendshapes for video: BOOK_neutral_location_repetitive_001_1_pmil\n",
      "Predicting occluded blendshapes for video: minister-flydde-fran-tomater-story_002_2_pmil\n"
     ]
    }
   ],
   "source": [
    "# Predict data for real occlusions on all of the data\n",
    "# Save it as blendshapes csv for each video\n",
    "\n",
    "\n",
    "def predict_data(model, data, mask, lengths):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        data = torch.tensor(data, dtype=torch.float32).to(device)\n",
    "        mask = torch.tensor(mask, dtype=torch.float32).to(device)\n",
    "        lengths = torch.tensor(lengths, dtype=torch.float32).to(device)\n",
    "        print(\"data.shape, mask.shape, lengths.shape: \", data.shape, mask.shape, lengths.shape)  # ([354, 51]) ([354]) ([354])\n",
    "        data_masked = data * mask.unsqueeze(-1)\n",
    "        data_masked = data_masked.to(device)\n",
    "        print(\"data_masked.shape: \", data_masked.shape)  # ([354, 51])\n",
    "        outputs = model(data_masked, lengths)\n",
    "    return outputs\n",
    "\n",
    "def save_blendshapes(outputs, video_name, blendshape_names, real_meta_keys, output_dir):\n",
    "    # outputs, video_name, blendshape_names, real_meta_keys, \"predicted_blendshapes\"\n",
    "    # outputs: F, 51\n",
    "    outputs = outputs.cpu().numpy()\n",
    "    frame_as_list = []\n",
    "\n",
    "    for i, frame in enumerate(outputs[0]):  # only one batch\n",
    "        frame_dict = {}\n",
    "\n",
    "        # take the rest of the keys from real data in video_blendshapes_cubic\n",
    "        frame_dict[\"Timecode\"] = real_meta_keys[i][\"Timecode\"]\n",
    "        frame_dict[\"BlendshapeCount\"] = real_meta_keys[i][\"BlendshapeCount\"]\n",
    "\n",
    "        for j in range(len(frame)):\n",
    "            frame_dict[blendshape_names[j]] = frame[j]\n",
    "\n",
    "        # frame_dict[\"velocity\"] = real_meta_keys[i][\"velocity\"]\n",
    "        # frame_dict[\"smoothed_velocity\"] = real_meta_keys[i][\"smoothed_velocity\"]\n",
    "        frame_as_list.append(frame_dict)\n",
    "    \n",
    "    # save as csv file: list of dicts\n",
    "    with open(f\"{output_dir}/{video_name}_blendshapes.csv\", \"w\") as f:\n",
    "        # write headers\n",
    "        f.write(\",\".join([\"Timecode\", \"BlendshapeCount\"] + blendshape_names) + \"\\n\")\n",
    "        for frame in frame_as_list:\n",
    "            f.write(\",\".join([f\"{frame[k]}\" for k in frame_dict.keys()]) + \"\\n\")\n",
    "\n",
    "\n",
    "# extract a list of blendshape names\n",
    "blendshape_names = list(rest_face_blendshapes.keys())\n",
    "print(blendshape_names)\n",
    "\n",
    "# read blendshapes from npz\n",
    "with np.load(\"blendshapes_timecodes_velocities_cubic_interpolated.npz\", allow_pickle=True) as data:\n",
    "    video_blendshapes_cubic = data[\"video_blendshapes\"].item()\n",
    "print(len(video_blendshapes_cubic))\n",
    "# Add occlusion labels to video_blendshapes_cubic\n",
    "video_blendshapes_cubic = add_occlusion_labels(video_blendshapes_cubic, video_occlusions_frames)\n",
    "print(video_blendshapes_cubic[\"varg_002_2_pmil\"][0][\"occluded\"])\n",
    "print(video_blendshapes_cubic[\"varg_002_2_pmil\"][0].keys())\n",
    "\n",
    "for video_name, video_frames in video_blendshapes_cubic.items():\n",
    "    print(f\"Predicting occluded blendshapes for video: {video_name}\")\n",
    "    data = []\n",
    "    mask = []\n",
    "    lengths = []\n",
    "    real_meta_keys = []\n",
    "    for frame in video_frames:\n",
    "        label = frame.pop(\"occluded\")   # 1 for occluded, 0 for not occluded\n",
    "        # reverse mask, to mask occluded data and keep the rest, otherwise I have 1 for occluded and 0 for not occluded\n",
    "        label = 1 - label\n",
    "        real_timecode = frame.pop(\"Timecode\", None)\n",
    "        real_blendshape_count = frame.pop(\"BlendshapeCount\", None)\n",
    "        real_velocity = frame.pop(\"velocity\", None)\n",
    "        real_smoothed_velocity = frame.pop(\"smoothed_velocity\", None)\n",
    "        real_meta_keys.append({\"Timecode\": real_timecode, \"BlendshapeCount\": real_blendshape_count, \"velocity\": real_velocity, \"smoothed_velocity\": real_smoothed_velocity})\n",
    "\n",
    "        data.append(list(frame.values()))\n",
    "        mask.append(label)\n",
    "        lengths.append(len(frame))\n",
    "\n",
    "    # check data length\n",
    "    try:\n",
    "        data = np.array(data)\n",
    "    except ValueError:\n",
    "        print(f\"Inhomogeneous number of blendshapes in {video_name}\")\n",
    "        new_data = []\n",
    "        for frame in data:\n",
    "            if len(frame) != 51:\n",
    "                frame = [0.0] * 51\n",
    "            new_data.append(frame)\n",
    "        data = np.array(new_data)\n",
    "\n",
    "    # outputs = predict_data(model, data, mask, lengths)\n",
    "    # Perform inference\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        data = torch.tensor(data, dtype=torch.float32).to(device)\n",
    "        mask = torch.tensor(mask, dtype=torch.float32).to(device)\n",
    "        lengths = torch.tensor(lengths, dtype=torch.float32).to(device)\n",
    "\n",
    "        sequence = data.unsqueeze(0).to(device)  # Add batch dimension\n",
    "        mask = mask.unsqueeze(0).to(device)  # Add batch dimension\n",
    "        lengths = lengths.unsqueeze(0).to(device)  # Add batch dimension\n",
    "        masked_sequence = sequence * mask.unsqueeze(-1)\n",
    "        outputs = model(masked_sequence, lengths).to(device)\n",
    "    \n",
    "    save_blendshapes(outputs, video_name, blendshape_names, real_meta_keys, \"predicted_blendshapes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
